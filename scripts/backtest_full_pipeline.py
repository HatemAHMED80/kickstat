#!/usr/bin/env python3
"""Full pipeline backtest: DC + ELO + XGBoost + Calibration + Bandit.

Compares 6 configurations side-by-side:
  1. BASELINE: Dixon-Coles + ELO (weighted average 65/35)
  2. +XGB:     Baseline + XGBoost stacking (all markets)
  3. +XGB_DRAW: XGB for draw only, DC+ELO for home/away
  4. +CAL:     +XGB + Isotonic Regression calibration
  5. +BANDIT:  +CAL + Contextual Bandit market selection
  6. HYBRID:   XGB draw + baseline (DC+ELO) for home/away + DC Poisson for O/U & AH

Walk-forward backtest with real Pinnacle + Max odds.

Usage:
    python scripts/backtest_full_pipeline.py
    python scripts/backtest_full_pipeline.py --leagues premier_league ligue_1
    python scripts/backtest_full_pipeline.py --leagues premier_league la_liga bundesliga serie_a ligue_1
    python scripts/backtest_full_pipeline.py --min-edge 3
    python scripts/backtest_full_pipeline.py --seasons 2022 2023 2024 2025
"""

import argparse
import json
import sys
import time
from collections import defaultdict
from pathlib import Path

import numpy as np
from loguru import logger

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.data.football_data_uk import load_historical_data, build_odds_lookup, build_multi_market_odds
from src.data.odds_api import remove_margin
from src.models.prop_models import remove_margin_2way
from src.models.dixon_coles import DixonColesModel, MatchResult
from src.models.elo import EloRating, EloMatch
from src.models.ensemble import EnsemblePredictor
from src.models.features import MatchHistory, compute_features, features_to_array
from src.models.xgb_model import XGBStackingModel
from src.models.xgb_props import XGBPropModel
from src.models.calibrator import ProbabilityCalibrator
from src.models.bandit import ContextualBandit
from src.evaluation.calibration import evaluate


LEAGUE_MARKET_STRATEGY: dict[str, dict[str, str]] = {
    "premier_league": {  # Total optimal: +116.4u, +7.3% ROI, 1605 bets
        "home": "baseline",      # +36.8u, +16.5%, 223 bets
        "draw": "xgb_draw",      # -6.1u, -20.2%, 30 bets
        "away": "xgb",           # +1.3u, +8.1%, 16 bets
        "over25": "xgb",         # +4.6u, +2.0%, 235 bets
        "under25": "baseline",   # +20.8u, +9.3%, 225 bets
        "ah_home": "baseline",   # +13.7u, +7.6%, 181 bets
        "ah_away": "baseline",   # +45.2u, +6.5%, 695 bets
    },
    "la_liga": {  # Total optimal: +72.8u, +6.8% ROI, 1075 bets
        "home": "xgb_cal",       # +18.7u, +6.3%, 296 bets
        "draw": "xgb_cal",       # +14.8u, +10.1%, 147 bets
        "away": "xgb",           # +1.3u, +1.8%, 75 bets
        "over25": "baseline",    # +18.5u, +11.1%, 167 bets
        "under25": "xgb",        # +6.0u, +3.5%, 170 bets
        "ah_home": "baseline",   # +14.1u, +11.5%, 123 bets
        "ah_away": "baseline",   # -18.7u (least bad)
    },
    "bundesliga": {
        "home": "xgb",           # +13.2u, +25.4%, 52 bets
        "draw": "xgb_cal",       # +3.4u, +2.2%, 155 bets
        "away": "xgb_cal",       # +7.5u, +15.1%, 50 bets
        "over25": "xgb",         # -1.0u (least bad)
        "under25": "baseline",   # +19.0u, +29.7%, 64 bets
        "ah_home": "baseline",   # +3.1u, +11.4%, 27 bets
        "ah_away": "baseline",   # -21.4u (least bad)
    },
    "serie_a": {  # Total optimal: +135.7u, +7.6% ROI, 1790 bets
        "home": "xgb_draw",      # +5.0u, +6.0%, 82 bets
        "draw": "xgb_cal",       # +18.1u, +6.9%, 260 bets
        "away": "xgb",           # -6.0u, -2.1%, 287 bets
        "over25": "xgb",         # +2.1u, +1.7%, 124 bets
        "under25": "baseline",   # +38.1u, +12.9%, 296 bets
        "ah_home": "baseline",   # +3.1u, +7.0%, 44 bets
        "ah_away": "baseline",   # +75.5u, +10.8%, 697 bets
    },
    "ligue_1": {  # Total optimal: +110.2u, +6.3% ROI, 1752 bets
        "home": "xgb",           # -1.3u, -3.1%, 41 bets
        "draw": "xgb",           # -4.5u, -3.9%, 117 bets
        "away": "baseline",      # +41.8u, +8.3%, 504 bets
        "over25": "xgb",         # -0.9u, -1.6%, 60 bets
        "under25": "baseline",   # +3.9u, +1.5%, 268 bets
        "ah_home": "baseline",   # +13.1u, +5.3%, 246 bets
        "ah_away": "baseline",   # +58.2u, +11.3%, 516 bets
    },
}

# Per-league overrides for edge thresholds and min probability filters.
# Only markets that need tighter filtering are listed; missing keys fall back to defaults.
LEAGUE_EDGE_OVERRIDES: dict[str, dict[str, float]] = {}

LEAGUE_MIN_PROB_OVERRIDES: dict[str, dict[str, float]] = {}

# Optimal config: per-league × per-market edge thresholds and min probs.
# Used ONLY for the "optimal" config. Overrides the shared market_thresholds.
# Generated by scripts/optimize_all.py grid search.
OPTIMAL_EDGE: dict[str, dict[str, float]] = {
    "premier_league": {
        "home": 3.0, "draw": 18.0, "away": 5.0,
        "over25": 16.0, "under25": 10.0,
        "ah_home": 18.0, "ah_away": 8.0,
    },
    "la_liga": {
        "home": 20.0, "draw": 13.0, "away": 11.0,
        "over25": 7.0, "under25": 20.0,
        "ah_home": 20.0, "ah_away": 3.0,
    },
    "bundesliga": {
        "home": 7.0, "draw": 16.0, "away": 14.0,
        "over25": 18.0, "under25": 3.0,
        "ah_home": 3.0, "ah_away": 3.0,
    },
    "serie_a": {
        "home": 12.0, "draw": 12.0, "away": 13.0,
        "over25": 16.0, "under25": 13.0,
        "ah_home": 3.0, "ah_away": 7.0,
    },
    "ligue_1": {
        "home": 16.0, "draw": 3.0, "away": 15.0,
        "over25": 19.0, "under25": 17.0,
        "ah_home": 11.0, "ah_away": 9.0,
    },
}

OPTIMAL_MIN_PROB: dict[str, dict[str, float]] = {
    "premier_league": {
        "home": 0.30, "draw": 0.25, "away": 0.65,
        "over25": 0.50, "under25": 0.55,
        "ah_home": 0.25, "ah_away": 0.25,
    },
    "la_liga": {
        "home": 0.45, "draw": 0.30, "away": 0.50,
        "over25": 0.65, "under25": 0.50,
        "ah_home": 0.60, "ah_away": 0.70,
    },
    "bundesliga": {
        "home": 0.65, "draw": 0.25, "away": 0.40,
        "over25": 0.65, "under25": 0.60,
        "ah_home": 0.70, "ah_away": 0.70,
    },
    "serie_a": {
        "home": 0.40, "draw": 0.25, "away": 0.30,
        "over25": 0.55, "under25": 0.25,
        "ah_home": 0.65, "ah_away": 0.25,
    },
    "ligue_1": {
        "home": 0.50, "draw": 0.25, "away": 0.25,
        "over25": 0.60, "under25": 0.35,
        "ah_home": 0.25, "ah_away": 0.55,
    },
}


def run_full_backtest(
    matches: list[dict],
    odds_data: dict,
    league: str = "unknown",
    min_training: int = 120,
    min_edge: float = 5.0,
    away_edge: float = 15.0,
    disable_away: bool = False,
    min_kelly: float = 1.0,
    kelly_fraction: float = 0.15,
    dc_weight: float = 0.65,
    elo_weight: float = 0.35,
    refit_interval: int = 30,
    xgb_retrain_interval: int = 60,
    calibrator_retrain_interval: int = 100,
    use_temp_scaling: bool = False,
    temp_scaling_t: float = 1.2,
) -> dict:
    """Run walk-forward backtest testing all pipeline configurations.

    Returns dict with results for each configuration.
    """
    matches = sorted(matches, key=lambda m: m["kickoff"])
    n = len(matches)
    logger.info(f"Starting FULL pipeline backtest on {n} matches, "
                f"{len(odds_data)} with odds")

    # Per-config accumulators
    configs = ["baseline", "xgb", "xgb_draw", "xgb_cal", "xgb_cal_bandit", "hybrid", "optimal"]
    all_probs = {c: [] for c in configs}
    all_outcomes = {c: [] for c in configs}
    bets = {c: [] for c in configs}
    edges_found = {c: 0 for c in configs}

    # Min probability filters per market (with per-league overrides)
    MARKET_MIN_PROB = {
        "home": 0.42,
        "draw": 0.28,
        "away": 0.40,
        "over25": 0.58,
        "under25": 0.50,
        "ah_home": 0.38,
        "ah_away": 0.55,
    }
    # Apply per-league min_prob overrides
    league_mp_overrides = LEAGUE_MIN_PROB_OVERRIDES.get(league, {})
    for mkt, val in league_mp_overrides.items():
        MARKET_MIN_PROB[mkt] = val

    # Per-league edge overrides (applied later in market_thresholds)
    league_edge_overrides = LEAGUE_EDGE_OVERRIDES.get(league, {})

    odds_matched = 0

    # Models
    elo = EloRating()
    history = MatchHistory()
    dc = None
    xgb_model = XGBStackingModel()
    calibrator = ProbabilityCalibrator(method="isotonic")
    bandit = ContextualBandit()

    # XGB prop models (binary classifiers for O/U 2.5 and AH)
    xgb_over25 = XGBPropModel(market_name="over25")
    xgb_ah = XGBPropModel(market_name="ah_home")

    # XGBoost training data (1X2)
    xgb_X: list[np.ndarray] = []
    xgb_y: list[int] = []

    # XGB prop training data
    prop_X: list[np.ndarray] = []
    prop_y_over25: list[int] = []
    prop_y_ah: list[tuple[int, float | None]] = []  # (label, ah_line) — line needed for AH

    # Calibrator training data (XGBoost predictions on validation)
    cal_probs_buffer: list[np.ndarray] = []
    cal_outcomes_buffer: list[int] = []

    # Bandit training data (matches with outcomes + odds)
    bandit_training: list[dict] = []

    # Initialize with first min_training matches
    current_season = matches[0].get("season") if matches else None
    for m in matches[:min_training]:
        # Detect season transition during init
        m_season = m.get("season")
        if m_season and current_season and m_season != current_season:
            elo.apply_seasonal_decay()
            logger.info(f"ELO seasonal decay applied: {current_season} → {m_season}")
            current_season = m_season
        elo.update(EloMatch(
            home_team=m["home_team"], away_team=m["away_team"],
            home_goals=m["home_score"], away_goals=m["away_score"],
        ))
        history.add_match(m)
        bandit_training.append(m)

    start_time = time.time()

    for i in range(min_training, n):
        test = matches[i]

        # Detect season transition
        test_season = test.get("season")
        if test_season and current_season and test_season != current_season:
            elo.apply_seasonal_decay()
            logger.info(f"ELO seasonal decay applied: {current_season} → {test_season}")
            current_season = test_season

        # === Phase 1: Update with PREVIOUS match ===
        if i > min_training:
            prev = matches[i - 1]

            # Compute features BEFORE adding to history (walk-forward safe)
            if dc is not None:
                try:
                    prev_features = compute_features(
                        prev["home_team"], prev["away_team"],
                        prev["kickoff"], history,
                        dc_model=dc, elo_model=elo,
                    )
                    prev_hs, prev_as = prev["home_score"], prev["away_score"]
                    prev_outcome = 0 if prev_hs > prev_as else (1 if prev_hs == prev_as else 2)
                    feat_arr = features_to_array(prev_features)
                    xgb_X.append(feat_arr)
                    xgb_y.append(prev_outcome)

                    # Prop labels
                    prop_X.append(feat_arr)
                    prop_y_over25.append(1 if (prev_hs + prev_as) > 2 else 0)

                    # If XGBoost is fitted, store its prediction for calibrator training
                    if xgb_model.is_fitted:
                        xgb_pred = xgb_model.model.predict_proba(
                            features_to_array(prev_features).reshape(1, -1)
                        )[0]
                        cal_probs_buffer.append(xgb_pred)
                        cal_outcomes_buffer.append(prev_outcome)
                except Exception:
                    pass

            # Now add to history and update ELO
            history.add_match(prev)
            elo.update(EloMatch(
                home_team=prev["home_team"], away_team=prev["away_team"],
                home_goals=prev["home_score"], away_goals=prev["away_score"],
            ))
            bandit_training.append(prev)

        # === Phase 2: Refit models ===

        # Dixon-Coles refit
        if dc is None or (i - min_training) % refit_interval == 0:
            dc_train = [
                MatchResult(
                    home_team=m["home_team"], away_team=m["away_team"],
                    home_goals=m["home_score"], away_goals=m["away_score"],
                    date=m["kickoff"],
                )
                for m in matches[:i]
            ]
            dc = DixonColesModel()
            try:
                dc.fit(dc_train)
            except ValueError:
                continue

        # XGBoost retrain (1X2)
        if (len(xgb_X) >= XGBStackingModel.MIN_TRAINING_SAMPLES
                and (i - min_training) % xgb_retrain_interval == 0):
            X_arr = np.array(xgb_X)
            y_arr = np.array(xgb_y)
            split = int(len(X_arr) * 0.8)
            xgb_model.fit(
                X_arr[:split], y_arr[:split],
                X_val=X_arr[split:], y_val=y_arr[split:],
            )

        # XGB prop retrain (O/U 2.5 and AH — same interval)
        if (len(prop_X) >= XGBPropModel.MIN_TRAINING_SAMPLES
                and (i - min_training) % xgb_retrain_interval == 0):
            pX = np.array(prop_X)
            split_p = int(len(pX) * 0.8)
            # Over 2.5
            py_o = np.array(prop_y_over25)
            xgb_over25.fit(pX[:split_p], py_o[:split_p],
                           X_val=pX[split_p:], y_val=py_o[split_p:])

        # Calibrator retrain
        if (len(cal_probs_buffer) >= 100
                and (i - min_training) % calibrator_retrain_interval == 0):
            cal_p = np.array(cal_probs_buffer)
            cal_o = np.array(cal_outcomes_buffer)
            calibrator = ProbabilityCalibrator(method="isotonic")
            calibrator.fit(cal_p, cal_o)

        # Bandit retrain (same interval as calibrator)
        if (len(bandit_training) >= 200
                and (i - min_training) % calibrator_retrain_interval == 0):
            bandit = ContextualBandit()
            bandit.fit(bandit_training, dc_model=dc, elo_model=elo)

        # Progress
        if (i - min_training) % 200 == 0:
            elapsed = time.time() - start_time
            logger.info(
                f"Progress: {i}/{n} ({elapsed:.0f}s) | "
                f"XGB samples={len(xgb_X)}, CAL samples={len(cal_probs_buffer)}, "
                f"Bandit={'ON' if bandit.is_fitted else 'OFF'}"
            )

        # === Phase 3: Predict with each configuration ===

        # Compute features for current match
        match_features = None
        if dc is not None:
            try:
                match_features = compute_features(
                    test["home_team"], test["away_team"],
                    test["kickoff"], history,
                    dc_model=dc, elo_model=elo,
                )
            except Exception:
                pass

        # Config 1: BASELINE (DC + ELO weighted average)
        ens_base = EnsemblePredictor(dc, elo, dc_weight, elo_weight)
        pred_base = ens_base.predict(test["home_team"], test["away_team"])
        probs_base = np.array([pred_base.home_prob, pred_base.draw_prob, pred_base.away_prob])

        # Config 2: +XGB (all markets)
        if xgb_model.is_fitted and match_features is not None:
            ens_xgb = EnsemblePredictor(
                dc, elo, dc_weight, elo_weight,
                xgb_model=xgb_model,
            )
            pred_xgb = ens_xgb.predict(
                test["home_team"], test["away_team"],
                match_features=match_features,
            )
            probs_xgb = np.array([pred_xgb.home_prob, pred_xgb.draw_prob, pred_xgb.away_prob])
        else:
            probs_xgb = probs_base.copy()

        # Config 2b: +XGB draw-only (XGB for draw, DC+ELO for home/away)
        if xgb_model.is_fitted and match_features is not None:
            ens_xgb_draw = EnsemblePredictor(
                dc, elo, dc_weight, elo_weight,
                xgb_model=xgb_model,
                xgb_markets={"draw"},
            )
            pred_xgb_draw = ens_xgb_draw.predict(
                test["home_team"], test["away_team"],
                match_features=match_features,
            )
            probs_xgb_draw = np.array([pred_xgb_draw.home_prob, pred_xgb_draw.draw_prob, pred_xgb_draw.away_prob])
        else:
            probs_xgb_draw = probs_base.copy()

        # Config 3: +XGB+CAL (apply calibrator post-hoc on XGB probs)
        if xgb_model.is_fitted and match_features is not None:
            probs_cal = probs_xgb.copy()
            if calibrator.is_fitted:
                try:
                    probs_cal = calibrator.calibrate(probs_xgb)
                except Exception:
                    probs_cal = probs_xgb.copy()
            if use_temp_scaling and temp_scaling_t > 0:
                logits = np.log(probs_cal + 1e-10)
                scaled = np.exp(logits / temp_scaling_t)
                probs_cal = scaled / scaled.sum()
        else:
            probs_cal = probs_xgb.copy()

        # Config 4: +XGB+CAL+BANDIT (same probs as config 3, but different bet selection)
        probs_bandit = probs_cal.copy()

        # Config 6: HYBRID (XGB draw + baseline home/away — same 1X2 probs as xgb_draw)
        probs_hybrid = probs_xgb_draw.copy()

        # Config 7: OPTIMAL — per-market best strategy for this league
        strategy = LEAGUE_MARKET_STRATEGY.get(league, {})
        source_map = {
            "baseline": probs_base,
            "xgb": probs_xgb,
            "xgb_draw": probs_xgb_draw,
            "xgb_cal": probs_cal,
        }
        probs_optimal = np.array([
            float(source_map.get(strategy.get("home", "baseline"), probs_base)[0]),
            float(source_map.get(strategy.get("draw", "baseline"), probs_base)[1]),
            float(source_map.get(strategy.get("away", "baseline"), probs_base)[2]),
        ])

        # Actual outcome
        hs, aws = test["home_score"], test["away_score"]
        outcome = 0 if hs > aws else (1 if hs == aws else 2)
        total_goals = hs + aws

        # Record predictions for calibration metrics
        for cfg, probs in [
            ("baseline", probs_base),
            ("xgb", probs_xgb),
            ("xgb_draw", probs_xgb_draw),
            ("xgb_cal", probs_cal),
            ("xgb_cal_bandit", probs_bandit),
            ("hybrid", probs_hybrid),
            ("optimal", probs_optimal),
        ]:
            all_probs[cfg].append(probs)
            all_outcomes[cfg].append(outcome)

        # === Phase 4: Edge calculation + betting simulation ===
        date_str = str(test["kickoff"])[:10]
        match_key = f"{test['home_team']}_vs_{test['away_team']}_{date_str}"

        if match_key not in odds_data:
            continue

        odds_matched += 1
        odds = odds_data[match_key]

        # Multi-market odds: check if we have the new format (dict with "1x2", "ou25", etc.)
        # Detect multi-market by checking for any known market key
        is_multi = any(isinstance(odds.get(k), dict) for k in ("1x2", "ou25", "ah", "corner_1x2", "corner_ou"))

        odds_1x2 = odds.get("1x2", {}) if is_multi else None
        has_1x2 = odds_1x2 and odds_1x2.get("pin_home", 0) > 1.0

        if has_1x2:
            fair = remove_margin(
                odds_1x2.get("pin_home", 0), odds_1x2.get("pin_draw", 0), odds_1x2.get("pin_away", 0)
            )
        elif not is_multi:
            fair = remove_margin(
                odds.get("home_odds", 0), odds.get("draw_odds", 0), odds.get("away_odds", 0)
            )
        else:
            # Multi-market but no 1X2 odds - set fair to empty so 1X2 is skipped
            fair = {"home": 0, "draw": 0, "away": 0}

        # Best available odds for 1X2
        if has_1x2:
            best_odds_map = {
                "home": odds_1x2.get("best_home", odds_1x2.get("pin_home", 0)),
                "draw": odds_1x2.get("best_draw", odds_1x2.get("pin_draw", 0)),
                "away": odds_1x2.get("best_away", odds_1x2.get("pin_away", 0)),
            }
        elif not is_multi:
            best_odds_map = {
                "home": odds.get("best_home", odds.get("home_odds", 0)),
                "draw": odds.get("best_draw", odds.get("draw_odds", 0)),
                "away": odds.get("best_away", odds.get("away_odds", 0)),
            }
        else:
            best_odds_map = {"home": 0, "draw": 0, "away": 0}

        # Market-specific edge thresholds (raised for losing markets)
        market_thresholds = {
            "home": max(min_edge, 8.0),
            "draw": min_edge,
            "away": 9999.0 if disable_away else away_edge,
            "over25": max(min_edge, 7.0),
            "under25": max(min_edge, 10.0),
            "ah_home": max(min_edge, 8.0),
            "ah_away": min_edge,
        }
        # Apply per-league edge overrides
        for mkt, val in league_edge_overrides.items():
            market_thresholds[mkt] = max(market_thresholds[mkt], val)

        # --- O/U 2.5 model probabilities ---
        dc_pred = dc.predict(test["home_team"], test["away_team"]) if dc else None
        # DC baseline probability for O/U 2.5
        dc_over25_p = dc_pred.over_25 if dc_pred is not None else None
        # XGB prop probability (if fitted)
        xgb_over25_p = None
        if xgb_over25.is_fitted and match_features is not None:
            xgb_over25_p = xgb_over25.predict_proba(match_features)

        # --- Asian Handicap spread probabilities from score_matrix ---
        ah_home_p = None  # P(home wins by AH line margin)
        ah_away_p = None
        ah_line = None
        if dc_pred is not None and dc_pred.score_matrix is not None:
            odds_ah = odds.get("ah") if is_multi else None
            if odds_ah:
                ah_line = odds_ah.get("line", 0)
                matrix = dc_pred.score_matrix
                n_goals = matrix.shape[0]
                # AH line is from home perspective (e.g., -1.5 means home must win by 2+)
                # Home covers: home_score + ah_line > away_score
                ah_home_p = 0.0
                for gi in range(n_goals):
                    for gj in range(n_goals):
                        if gi + ah_line > gj:
                            ah_home_p += float(matrix[gi, gj])
                ah_away_p = 1.0 - ah_home_p

        # For configs 1-4 + hybrid: standard edge-based betting (1X2)
        for cfg, probs in [
            ("baseline", probs_base),
            ("xgb", probs_xgb),
            ("xgb_draw", probs_xgb_draw),
            ("xgb_cal", probs_cal),
            ("hybrid", probs_hybrid),
            ("optimal", probs_optimal),
        ]:
            # --- 1X2 markets ---
            for market_idx, market_name in enumerate(["home", "draw", "away"]):
                model_p = float(probs[market_idx])
                fair_p = fair[market_name]
                best_o = best_odds_map[market_name]

                if fair_p <= 0 or best_o <= 1.0:
                    continue

                # Min probability filter (optimal uses its own thresholds)
                if cfg == "optimal":
                    eff_min_prob = OPTIMAL_MIN_PROB.get(league, {}).get(market_name, MARKET_MIN_PROB.get(market_name, 0))
                    eff_edge_thr = OPTIMAL_EDGE.get(league, {}).get(market_name, market_thresholds[market_name])
                else:
                    eff_min_prob = MARKET_MIN_PROB.get(market_name, 0)
                    eff_edge_thr = market_thresholds[market_name]

                if model_p < eff_min_prob:
                    continue

                edge = ((model_p - fair_p) / fair_p) * 100

                if edge >= eff_edge_thr:
                    # Kelly filter: only bet if Kelly stake >= min_kelly
                    b = best_o - 1.0
                    q = 1.0 - model_p
                    kelly_raw = (b * model_p - q) / b if b > 0 else 0
                    kelly_pct = max(0, kelly_raw * kelly_fraction * 100)

                    if kelly_pct < min_kelly:
                        continue

                    edges_found[cfg] += 1
                    won = outcome == market_idx
                    pnl = (best_o - 1.0) if won else -1.0

                    bets[cfg].append({
                        "date": date_str,
                        "home": test["home_team"],
                        "away": test["away_team"],
                        "market": market_name,
                        "model_prob": round(model_p, 4),
                        "fair_prob": round(fair_p, 4),
                        "edge_pct": round(edge, 1),
                        "kelly_pct": round(kelly_pct, 1),
                        "best_odds": best_o,
                        "outcome": ["home", "draw", "away"][outcome],
                        "won": won,
                        "pnl": round(pnl, 2),
                    })

            # --- Over/Under 2.5 ---
            odds_ou = odds.get("ou25") if is_multi else None
            # Baseline and hybrid use DC Poisson, XGB configs use XGB prop (fallback to DC)
            if cfg in ("baseline", "hybrid"):
                over25_model_p = dc_over25_p
            elif cfg == "optimal":
                # Optimal can use different sources for over25 vs under25
                over_src = strategy.get("over25", "baseline")
                if over_src == "baseline":
                    over25_model_p = dc_over25_p
                else:
                    over25_model_p = xgb_over25_p if xgb_over25_p is not None else dc_over25_p
            else:
                over25_model_p = xgb_over25_p if xgb_over25_p is not None else dc_over25_p

            if cfg == "optimal":
                under_src = strategy.get("under25", "baseline")
                if under_src == "baseline":
                    under25_model_p = (1.0 - dc_over25_p) if dc_over25_p is not None else None
                else:
                    _xgb_p = xgb_over25_p if xgb_over25_p is not None else dc_over25_p
                    under25_model_p = (1.0 - _xgb_p) if _xgb_p is not None else None
            else:
                under25_model_p = (1.0 - over25_model_p) if over25_model_p is not None else None
            if odds_ou and over25_model_p is not None:
                fair_over, fair_under = remove_margin_2way(
                    odds_ou["pin_over"], odds_ou["pin_under"]
                )
                for mkt_name, model_p, fair_p, best_key, won_cond in [
                    ("over25", over25_model_p, fair_over, "best_over", total_goals > 2),
                    ("under25", under25_model_p, fair_under, "best_under", total_goals < 3),
                ]:
                    best_o = odds_ou.get(best_key, 0)
                    if fair_p <= 0 or best_o <= 1.0:
                        continue
                    if cfg == "optimal":
                        ou_min_prob = OPTIMAL_MIN_PROB.get(league, {}).get(mkt_name, MARKET_MIN_PROB.get(mkt_name, 0))
                        ou_edge_thr = OPTIMAL_EDGE.get(league, {}).get(mkt_name, market_thresholds[mkt_name])
                    else:
                        ou_min_prob = MARKET_MIN_PROB.get(mkt_name, 0)
                        ou_edge_thr = market_thresholds[mkt_name]
                    if model_p < ou_min_prob:
                        continue
                    edge = ((model_p - fair_p) / fair_p) * 100
                    if edge >= ou_edge_thr:
                        b = best_o - 1.0
                        q = 1.0 - model_p
                        kelly_raw = (b * model_p - q) / b if b > 0 else 0
                        kelly_pct = max(0, kelly_raw * kelly_fraction * 100)
                        if kelly_pct < min_kelly:
                            continue
                        edges_found[cfg] += 1
                        pnl = (best_o - 1.0) if won_cond else -1.0
                        bets[cfg].append({
                            "date": date_str,
                            "home": test["home_team"],
                            "away": test["away_team"],
                            "market": mkt_name,
                            "model_prob": round(model_p, 4),
                            "fair_prob": round(fair_p, 4),
                            "edge_pct": round(edge, 1),
                            "kelly_pct": round(kelly_pct, 1),
                            "best_odds": best_o,
                            "outcome": f"goals={total_goals}",
                            "won": won_cond,
                            "pnl": round(pnl, 2),
                        })

            # --- Asian Handicap ---
            odds_ah = odds.get("ah") if is_multi else None
            if odds_ah and ah_home_p is not None and ah_line is not None:
                fair_ahh, fair_aha = remove_margin_2way(
                    odds_ah["pin_home"], odds_ah["pin_away"]
                )
                goal_diff = hs - aws
                # Home covers AH: home_score + ah_line > away_score
                ah_home_won = (goal_diff + ah_line) > 0
                ah_away_won = (goal_diff + ah_line) < 0
                # If exactly 0 = push (refund), skip
                is_push = (goal_diff + ah_line) == 0

                if not is_push:
                    for mkt_name, model_p, fair_p, best_key, won_cond in [
                        ("ah_home", ah_home_p, fair_ahh, "best_home", ah_home_won),
                        ("ah_away", ah_away_p, fair_aha, "best_away", ah_away_won),
                    ]:
                        best_o = odds_ah.get(best_key, 0)
                        if fair_p <= 0 or best_o <= 1.0:
                            continue
                        if cfg == "optimal":
                            ah_min_prob = OPTIMAL_MIN_PROB.get(league, {}).get(mkt_name, MARKET_MIN_PROB.get(mkt_name, 0))
                            ah_edge_thr = OPTIMAL_EDGE.get(league, {}).get(mkt_name, market_thresholds[mkt_name])
                        else:
                            ah_min_prob = MARKET_MIN_PROB.get(mkt_name, 0)
                            ah_edge_thr = market_thresholds[mkt_name]
                        if model_p < ah_min_prob:
                            continue
                        edge = ((model_p - fair_p) / fair_p) * 100
                        if edge >= ah_edge_thr:
                            b = best_o - 1.0
                            q = 1.0 - model_p
                            kelly_raw = (b * model_p - q) / b if b > 0 else 0
                            kelly_pct = max(0, kelly_raw * kelly_fraction * 100)
                            if kelly_pct < min_kelly:
                                continue
                            edges_found[cfg] += 1
                            pnl = (best_o - 1.0) if won_cond else -1.0
                            bets[cfg].append({
                                "date": date_str,
                                "home": test["home_team"],
                                "away": test["away_team"],
                                "market": mkt_name,
                                "ah_line": ah_line,
                                "model_prob": round(model_p, 4),
                                "fair_prob": round(fair_p, 4),
                                "edge_pct": round(edge, 1),
                                "kelly_pct": round(kelly_pct, 1),
                                "best_odds": best_o,
                                "outcome": f"diff={goal_diff}",
                                "won": won_cond,
                                "pnl": round(pnl, 2),
                            })

        # Config 5: xgb_cal_bandit — bandit is a stub, so it mirrors xgb_cal
        # Bets are generated via the main loop above (xgb_cal already in the loop)
        # Copy xgb_cal bets for this match to xgb_cal_bandit
        match_cal_bets = [b for b in bets["xgb_cal"] if b["date"] == date_str
                          and b["home"] == test["home_team"]
                          and b["away"] == test["away_team"]]
        for b in match_cal_bets:
            bets["xgb_cal_bandit"].append(b.copy())
            edges_found["xgb_cal_bandit"] += 1

    elapsed = time.time() - start_time

    # === Aggregate results per config ===
    results = {
        "elapsed_seconds": round(elapsed, 1),
        "total_matches": n,
        "matches_tested": n - min_training,
        "matches_with_odds": odds_matched,
        "configs": {},
    }

    for cfg in configs:
        probs_arr = np.array(all_probs[cfg]) if all_probs[cfg] else np.empty((0, 3))
        outcomes_arr = np.array(all_outcomes[cfg]) if all_outcomes[cfg] else np.empty(0, dtype=int)

        # Calibration metrics
        if len(probs_arr) >= 10:
            cal_report = evaluate(probs_arr, outcomes_arr)
            cal_dict = {
                "brier_score": round(cal_report.brier_score, 4),
                "log_loss": round(cal_report.log_loss, 4),
                "ece": round(cal_report.ece, 4),
                "accuracy": round(cal_report.accuracy, 4),
                "is_acceptable": cal_report.is_acceptable,
                "n_predictions": cal_report.n_predictions,
            }
        else:
            cal_dict = {"error": "insufficient predictions"}

        # Betting stats
        cfg_bets = bets[cfg]
        n_bets = len(cfg_bets)
        wins = sum(1 for b in cfg_bets if b["won"])
        total_pnl = sum(b["pnl"] for b in cfg_bets)
        roi = total_pnl / n_bets if n_bets > 0 else 0

        # By market
        by_market = {}
        all_market_names = set(b["market"] for b in cfg_bets)
        for market in sorted(all_market_names):
            mb = [b for b in cfg_bets if b["market"] == market]
            if mb:
                m_pnl = sum(b["pnl"] for b in mb)
                m_wins = sum(1 for b in mb if b["won"])
                m_edges = [b["edge_pct"] for b in mb]
                by_market[market] = {
                    "bets": len(mb),
                    "wins": m_wins,
                    "win_rate": round(m_wins / len(mb), 4),
                    "pnl": round(m_pnl, 2),
                    "roi": round(m_pnl / len(mb), 4),
                    "avg_edge": round(np.mean(m_edges), 1),
                }

        # Cumulative PnL curve (for plotting)
        cum_pnl = []
        running = 0.0
        for b in cfg_bets:
            running += b["pnl"]
            cum_pnl.append(round(running, 2))

        results["configs"][cfg] = {
            "calibration": cal_dict,
            "betting": {
                "total_bets": n_bets,
                "wins": wins,
                "win_rate": round(wins / n_bets, 4) if n_bets > 0 else 0,
                "total_pnl": round(total_pnl, 2),
                "roi": round(roi, 4),
                "avg_edge": round(np.mean([b["edge_pct"] for b in cfg_bets]), 1) if cfg_bets else 0,
                "by_market": by_market,
            },
            "cumulative_pnl": cum_pnl,
            "bets": cfg_bets,
        }

    # XGBoost feature importance
    if xgb_model.is_fitted:
        results["xgb_info"] = {
            "train_samples": xgb_model._n_train_samples,
            "feature_importance": {
                k: round(v, 4) for k, v in
                list(xgb_model.feature_importance().items())[:15]
            },
        }

    # Bandit segment summary
    if bandit.is_fitted:
        results["bandit_info"] = {
            "segments": len(bandit.get_segment_summary()),
            "summary": {
                seg: {
                    "best_arm": info["best_arm"],
                    "best_roi": info["best_roi"],
                    "n_arms": len(info.get("arms", {})),
                }
                for seg, info in bandit.get_segment_summary().items()
            },
        }

    return results


def print_report(results: dict) -> None:
    """Print formatted comparison report."""
    print("\n" + "=" * 75)
    print("  FULL PIPELINE BACKTEST - CONFIGURATION COMPARISON")
    print("=" * 75)
    print(f"  Matches: {results['matches_tested']} tested, "
          f"{results['matches_with_odds']} with odds | "
          f"Time: {results['elapsed_seconds']:.0f}s")

    config_names = {
        "baseline": "DC + ELO (baseline)",
        "xgb": "+ XGBoost (all markets)",
        "xgb_draw": "+ XGBoost (draw only)",
        "xgb_cal": "+ XGB + Calibration",
        "xgb_cal_bandit": "+ XGB + CAL + Bandit",
        "hybrid": "HYBRID (XGB draw + base)",
        "optimal": "OPTIMAL (best/market)",
    }

    # Use configs from results keys
    configs = list(results["configs"].keys())

    # Header
    print(f"\n  {'Config':<30} {'Brier':>8} {'ECE':>8} {'Acc':>8} "
          f"{'Bets':>6} {'Win%':>7} {'PnL':>8} {'ROI':>8}")
    print("  " + "-" * 93)

    for cfg in configs:
        data = results["configs"].get(cfg, {})
        cal = data.get("calibration", {})
        bet = data.get("betting", {})

        name = config_names.get(cfg, cfg)
        brier = cal.get("brier_score", 0)
        ece = cal.get("ece", 0)
        acc = cal.get("accuracy", 0)
        n_bets = bet.get("total_bets", 0)
        wr = bet.get("win_rate", 0)
        pnl = bet.get("total_pnl", 0)
        roi = bet.get("roi", 0)

        pnl_str = f"{pnl:+.1f}u"
        roi_str = f"{roi:+.1%}"

        print(f"  {name:<30} {brier:>8.4f} {ece:>8.4f} {acc:>7.1%} "
              f"{n_bets:>6} {wr:>6.1%} {pnl_str:>8} {roi_str:>8}")

    # Delta analysis
    print(f"\n  {'='*75}")
    print("  DELTA ANALYSIS (vs baseline)")
    print(f"  {'-'*75}")

    base_cal = results["configs"]["baseline"]["calibration"]
    base_bet = results["configs"]["baseline"]["betting"]

    for cfg in [c for c in configs if c != "baseline"]:
        data = results["configs"][cfg]
        cal = data["calibration"]
        bet = data["betting"]

        name = config_names.get(cfg, cfg)
        d_brier = cal.get("brier_score", 0) - base_cal.get("brier_score", 0)
        d_ece = cal.get("ece", 0) - base_cal.get("ece", 0)
        d_roi = bet.get("roi", 0) - base_bet.get("roi", 0)
        d_pnl = bet.get("total_pnl", 0) - base_bet.get("total_pnl", 0)

        brier_arrow = "v" if d_brier < 0 else "^"  # lower is better
        ece_arrow = "v" if d_ece < 0 else "^"
        roi_arrow = "^" if d_roi > 0 else "v"

        print(f"  {name:<30} Brier {d_brier:+.4f}{brier_arrow} | "
              f"ECE {d_ece:+.4f}{ece_arrow} | "
              f"ROI {d_roi:+.1%}{roi_arrow} | "
              f"PnL {d_pnl:+.1f}u")

    # By-market breakdown for best config
    print(f"\n  {'='*75}")
    print("  PER-MARKET BREAKDOWN (each config)")
    print(f"  {'-'*75}")

    for cfg in configs:
        name = config_names.get(cfg, cfg)
        by_market = results["configs"][cfg]["betting"].get("by_market", {})
        if not by_market:
            print(f"  {name}: no bets")
            continue
        print(f"\n  {name}:")
        for market, stats in by_market.items():
            arrow = "+" if stats["pnl"] >= 0 else ""
            print(f"    {market:>5}: {stats['bets']:3d} bets | "
                  f"win {stats['win_rate']:.0%} | "
                  f"PnL {arrow}{stats['pnl']:.1f}u | "
                  f"ROI {stats['roi']:+.1%} | "
                  f"avg edge {stats['avg_edge']:.1f}%")

    # Feature importance
    if "xgb_info" in results:
        print(f"\n  {'='*75}")
        print(f"  TOP FEATURES (XGB trained on {results['xgb_info']['train_samples']} samples)")
        print(f"  {'-'*75}")
        for feat, imp in list(results["xgb_info"]["feature_importance"].items())[:10]:
            bar = "#" * int(imp * 120)
            print(f"    {feat:>30}: {imp:.4f} {bar}")

    # Bandit info
    if "bandit_info" in results:
        print(f"\n  {'='*75}")
        print(f"  BANDIT SEGMENTS ({results['bandit_info']['segments']} segments)")
        print(f"  {'-'*75}")
        for seg, info in list(results["bandit_info"]["summary"].items())[:6]:
            print(f"    {seg:<35} -> {info['best_arm']} (ROI: {info['best_roi']:.1f}%)")

    # Verdict
    print(f"\n  {'='*75}")

    # Find best config by ROI
    best_cfg = max(
        configs,
        key=lambda c: results["configs"][c]["betting"].get("roi", -999),
    )
    best_roi = results["configs"][best_cfg]["betting"]["roi"]
    best_pnl = results["configs"][best_cfg]["betting"]["total_pnl"]
    best_cal = results["configs"][best_cfg]["calibration"]

    cal_ok = best_cal.get("is_acceptable", False)
    sample_ok = results["configs"][best_cfg]["betting"]["total_bets"] >= 50
    roi_ok = best_roi > -0.05

    if cal_ok and sample_ok and roi_ok:
        if best_roi > 0:
            verdict = f"GO - {config_names.get(best_cfg, best_cfg)} shows +ROI ({best_roi:+.1%}, {best_pnl:+.1f}u)"
        else:
            verdict = f"CAUTIOUS - Best is {config_names.get(best_cfg, best_cfg)} ({best_roi:+.1%})"
    else:
        reasons = []
        if not cal_ok:
            reasons.append("calibration failed")
        if not sample_ok:
            reasons.append("insufficient bets")
        if not roi_ok:
            reasons.append(f"ROI too negative ({best_roi:+.1%})")
        verdict = f"NO-GO - {', '.join(reasons)}"

    print(f"  VERDICT: {verdict}")
    print("=" * 75)


def run_single_league_backtest(league: str, args) -> dict | None:
    """Run backtest for a single league and return results."""
    cache_dir = PROJECT_ROOT / "data" / "historical"

    logger.info(f"\n{'='*60}")
    logger.info(f"  BACKTEST: {league}")
    logger.info(f"{'='*60}")

    # Load data
    logger.info(f"Loading {league} seasons {args.seasons}")
    matches = load_historical_data(league, args.seasons, cache_dir)

    if len(matches) < args.min_training + 50:
        print(f"  [SKIP] {league}: Only {len(matches)} matches (need {args.min_training + 50})")
        return None

    # Build odds lookup (multi-market: 1X2 + O/U 2.5 + Asian Handicap)
    odds_data = build_multi_market_odds(matches)

    # Data quality
    has_pinnacle = sum(1 for m in matches if m.get("pinnacle_home", 0) > 1.0)
    has_max = sum(1 for m in matches if m.get("max_home", 0) > 1.0)
    logger.info(f"Data: {len(matches)} matches | Pinnacle: {has_pinnacle} | Max: {has_max}")

    # Run backtest
    results = run_full_backtest(
        matches, odds_data,
        league=league,
        min_training=args.min_training,
        min_edge=args.min_edge,
        away_edge=args.away_edge,
        disable_away=args.disable_away,
        min_kelly=args.min_kelly,
        kelly_fraction=args.kelly_fraction,
        refit_interval=args.refit_interval,
        xgb_retrain_interval=args.xgb_retrain,
        use_temp_scaling=args.temp_scaling,
        temp_scaling_t=args.temp_t,
    )

    results["league"] = league
    return results


def main():
    parser = argparse.ArgumentParser(description="Full pipeline backtest")
    parser.add_argument(
        "--seasons", nargs="+", type=int, default=[2021, 2022, 2023, 2024, 2025],
        help="Seasons to include (default: 2021-2025)"
    )
    parser.add_argument(
        "--leagues", nargs="+", default=["premier_league"],
        help="Leagues to backtest (default: premier_league). "
             "Use 'all' for all 5 top leagues."
    )
    # Keep --league for backwards compatibility
    parser.add_argument("--league", default=None,
                        help="Single league (deprecated, use --leagues)")
    parser.add_argument("--min-edge", type=float, default=5.0,
                        help="Min edge %% for home/draw betting (default: 5)")
    parser.add_argument("--away-edge", type=float, default=15.0,
                        help="Min edge %% for away betting (default: 15)")
    parser.add_argument("--disable-away", action="store_true",
                        help="Completely disable away market betting")
    parser.add_argument("--min-kelly", type=float, default=1.0,
                        help="Min Kelly stake %% to qualify (default: 1.0)")
    parser.add_argument("--kelly-fraction", type=float, default=0.15,
                        help="Kelly fraction for stake sizing (default: 0.15 = sixth Kelly)")
    parser.add_argument("--min-training", type=int, default=120,
                        help="Min training matches before testing")
    parser.add_argument("--refit-interval", type=int, default=30,
                        help="DC refit interval")
    parser.add_argument("--xgb-retrain", type=int, default=60,
                        help="XGB retrain interval")
    parser.add_argument("--temp-scaling", action="store_true",
                        help="Use temperature scaling instead of isotonic calibration")
    parser.add_argument("--temp-t", type=float, default=1.2,
                        help="Temperature for temp scaling (default: 1.2)")
    args = parser.parse_args()

    # Resolve leagues
    ALL_TOP_5 = ["premier_league", "ligue_1", "la_liga", "bundesliga", "serie_a"]

    if args.league:
        # Backwards compat: --league overrides --leagues
        leagues = [args.league]
    elif "all" in args.leagues:
        leagues = ALL_TOP_5
    else:
        leagues = args.leagues

    results_dir = PROJECT_ROOT / "data" / "results"
    results_dir.mkdir(parents=True, exist_ok=True)

    all_results = {}

    for league in leagues:
        results = run_single_league_backtest(league, args)
        if results is None:
            continue

        all_results[league] = results

        # Print report
        print_report(results)

        # Save per-league summary
        summary = {
            "league": league,
            "elapsed_seconds": results["elapsed_seconds"],
            "total_matches": results["total_matches"],
            "matches_tested": results["matches_tested"],
            "matches_with_odds": results["matches_with_odds"],
            "configs": {},
        }
        for cfg, data in results["configs"].items():
            summary["configs"][cfg] = {
                "calibration": data["calibration"],
                "betting": {k: v for k, v in data["betting"].items()},
            }
        if "xgb_info" in results:
            summary["xgb_info"] = results["xgb_info"]
        if "bandit_info" in results:
            summary["bandit_info"] = results["bandit_info"]

        summary_path = results_dir / f"backtest_{league}_summary.json"
        with open(summary_path, "w") as f:
            json.dump(summary, f, indent=2, default=str)
        print(f"\n  Summary saved to {summary_path}")

        # Save bets per config
        for cfg in results["configs"]:
            cfg_bets = results["configs"][cfg].get("bets", [])
            if cfg_bets:
                bets_path = results_dir / f"backtest_{league}_{cfg}_bets.json"
                with open(bets_path, "w") as f:
                    json.dump(cfg_bets, f, indent=2, default=str)

    # If multiple leagues, print cross-league summary
    if len(all_results) > 1:
        print(f"\n\n{'='*80}")
        print("  CROSS-LEAGUE SUMMARY")
        print(f"{'='*80}")
        print(f"\n  {'League':<20} {'Matches':>8} {'Best Config':<28} {'ROI':>8} {'PnL':>8} {'Bets':>6}")
        print(f"  {'-'*80}")

        total_pnl = 0.0
        total_bets = 0

        for league, results in all_results.items():
            best_cfg = max(
                results["configs"],
                key=lambda c: results["configs"][c]["betting"].get("roi", -999),
            )
            bet = results["configs"][best_cfg]["betting"]
            roi = bet.get("roi", 0)
            pnl = bet.get("total_pnl", 0)
            n_bets = bet.get("total_bets", 0)
            total_pnl += pnl
            total_bets += n_bets

            config_names = {
                "baseline": "DC + ELO",
                "xgb": "+ XGBoost",
                "xgb_draw": "+ XGB draw",
                "xgb_cal": "+ Calibration",
                "xgb_cal_bandit": "+ Bandit",
                "hybrid": "HYBRID",
                "optimal": "OPTIMAL",
            }
            print(f"  {league:<20} {results['matches_tested']:>8} "
                  f"{config_names.get(best_cfg, best_cfg):<28} "
                  f"{roi:>+7.1%} {pnl:>+7.1f}u {n_bets:>6}")

        overall_roi = total_pnl / total_bets if total_bets > 0 else 0
        print(f"  {'-'*80}")
        print(f"  {'TOTAL':<20} {'':>8} {'':>28} "
              f"{overall_roi:>+7.1%} {total_pnl:>+7.1f}u {total_bets:>6}")
        print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
