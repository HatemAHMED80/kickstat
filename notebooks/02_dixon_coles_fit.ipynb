{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Ajustement du modele Dixon-Coles\n",
    "\n",
    "**Objectif** : Fitter le modele Dixon-Coles sur les donnees historiques de Ligue 1 et verifier que les parametres sont coherents.\n",
    "\n",
    "## Ce qu'on verifie :\n",
    "1. Le MLE converge correctement\n",
    "2. Les ratings attack/defense sont coherents avec le classement reel\n",
    "3. Le parametre rho est dans les bornes attendues (-0.3 a 0)\n",
    "4. Le home advantage est realiste (0.1 a 0.4)\n",
    "5. Les predictions de score sont sensees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from src.models.dixon_coles import DixonColesModel, MatchResult\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Charger les donnees d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger saison 2023-24 (entrainement)\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "df = pd.read_csv(DATA_DIR / 'ligue1_2023.csv')\n",
    "df['kickoff'] = pd.to_datetime(df['kickoff'])\n",
    "print(f'Matchs charges: {len(df)}')\n",
    "print(f'Equipes: {df[\"home_team\"].nunique()}')\n",
    "print(f'Periode: {df[\"kickoff\"].min().date()} -> {df[\"kickoff\"].max().date()}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convertir en MatchResult et fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Convertir en MatchResult\n",
    "match_results = [\n",
    "    MatchResult(\n",
    "        home_team=row['home_team'],\n",
    "        away_team=row['away_team'],\n",
    "        home_goals=int(row['home_score']),\n",
    "        away_goals=int(row['away_score']),\n",
    "        date=row['kickoff'].to_pydatetime(),\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "print(f'Matchs pour fitting: {len(match_results)}')\n",
    "\n",
    "# Fitter le modele\n",
    "model = DixonColesModel(half_life_days=180)\n",
    "model.fit(match_results)\n",
    "\n",
    "print(f'\\nConvergence: {model._convergence_info}')\n",
    "print(f'Home advantage: {model.home_advantage:.3f}')\n",
    "print(f'Rho: {model.rho:.3f}')\n",
    "print(f'Avg goals: {model.avg_goals:.3f}')\n",
    "print(f'Equipes fittees: {len(model.teams)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classement des equipes par force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classement par ratio attack/defense\n",
    "rankings = model.get_team_rankings()\n",
    "df_rank = pd.DataFrame(rankings)\n",
    "print('\\nClassement par force (attack/defense ratio):\\n')\n",
    "print(df_rank.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation attack vs defense\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for _, row in df_rank.iterrows():\n",
    "    ax.scatter(row['attack'], row['defense'], s=100, zorder=5)\n",
    "    ax.annotate(row['team'], (row['attack'], row['defense']),\n",
    "                textcoords='offset points', xytext=(5, 5), fontsize=8)\n",
    "\n",
    "ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Attack Rating (> 1 = above average)')\n",
    "ax.set_ylabel('Defense Rating (< 1 = better defense)')\n",
    "ax.set_title('Dixon-Coles: Attack vs Defense Ratings - Ligue 1')\n",
    "\n",
    "# Quadrant labels\n",
    "ax.text(0.98, 0.02, 'Fort attaque\\nForte defense', transform=ax.transAxes,\n",
    "        ha='right', va='bottom', fontsize=9, color='green', alpha=0.7)\n",
    "ax.text(0.02, 0.98, 'Faible attaque\\nFaible defense', transform=ax.transAxes,\n",
    "        ha='left', va='top', fontsize=9, color='red', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_ROOT / 'data' / 'results' / 'team_ratings.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test de prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predire quelques matchs pour verifier la coherence\n",
    "test_matchups = [\n",
    "    ('Paris Saint-Germain', 'Montpellier HSC'),   # Gros favori\n",
    "    ('Paris Saint-Germain', 'AS Monaco'),          # Choc\n",
    "    ('Montpellier HSC', 'Le Havre AC'),            # Bas de tableau\n",
    "]\n",
    "\n",
    "teams_available = list(model.teams.keys())\n",
    "print('Equipes disponibles:', teams_available[:5], '...')\n",
    "print()\n",
    "\n",
    "for home, away in test_matchups:\n",
    "    # Trouver les noms les plus proches si exact match echoue\n",
    "    if home not in model.teams or away not in model.teams:\n",
    "        print(f'\\n[SKIP] {home} vs {away} - equipe non trouvee')\n",
    "        continue\n",
    "    \n",
    "    pred = model.predict(home, away)\n",
    "    d = pred.to_dict()\n",
    "    print(f\"\\n{home} vs {away}\")\n",
    "    print(f\"  Lambda: {d['lambda_home']:.2f} - {d['lambda_away']:.2f}\")\n",
    "    print(f\"  1X2: {d['1x2']['home']:.1%} / {d['1x2']['draw']:.1%} / {d['1x2']['away']:.1%}\")\n",
    "    print(f\"  O/U 2.5: {d['over_under']['over_25']:.1%} / {1-d['over_under']['over_25']:.1%}\")\n",
    "    print(f\"  BTTS: {d['btts']['yes']:.1%} / {d['btts']['no']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verification rapide sur les matchs termines\n",
    "\n",
    "On predit les matchs de la saison d'entrainement (in-sample) pour verifier que le modele n'est pas completement a cote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction in-sample (attention: c'est du in-sample, pas du out-of-sample!)\n",
    "correct = 0\n",
    "total = 0\n",
    "probs_home = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    h, a = row['home_team'], row['away_team']\n",
    "    if h not in model.teams or a not in model.teams:\n",
    "        continue\n",
    "    \n",
    "    pred = model.predict(h, a)\n",
    "    predicted = max(\n",
    "        [('H', pred.home_win), ('D', pred.draw), ('A', pred.away_win)],\n",
    "        key=lambda x: x[1]\n",
    "    )[0]\n",
    "    \n",
    "    # Resultat reel\n",
    "    hs, as_ = int(row['home_score']), int(row['away_score'])\n",
    "    actual = 'H' if hs > as_ else ('D' if hs == as_ else 'A')\n",
    "    \n",
    "    if predicted == actual:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    probs_home.append(pred.home_win)\n",
    "\n",
    "print(f'Accuracy in-sample: {correct/total:.1%} ({correct}/{total})')\n",
    "print(f'(Attention: c est du in-sample, le vrai test est dans notebook 05_backtest)')\n",
    "print(f'\\nProba home win moyenne: {np.mean(probs_home):.3f}')\n",
    "print(f'Home win rate reel:     {(df[\"home_score\"] > df[\"away_score\"]).mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prochaine etape\n",
    "\n",
    "Le modele Dixon-Coles est fitte. Passons au notebook **03** pour le modele ELO, puis au **05_backtest** pour la validation walk-forward."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
