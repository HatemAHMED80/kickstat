{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Systeme ELO\n",
    "\n",
    "**Objectif** : Construire les ratings ELO sur l'historique Ligue 1 et verifier la coherence.\n",
    "\n",
    "Le systeme ELO est notre deuxieme modele (poids 35% dans l'ensemble).\n",
    "Il capture la force relative des equipes de facon incrementale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.models.elo import EloRating, EloMatch\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les matchs\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "df = pd.read_csv(DATA_DIR / 'ligue1_2023.csv')\n",
    "df['kickoff'] = pd.to_datetime(df['kickoff'])\n",
    "df = df.sort_values('kickoff')\n",
    "print(f'Matchs: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construire les ratings ELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo = EloRating(k_factor=32, home_advantage=100)\n",
    "\n",
    "# Historique pour tracer l'evolution\n",
    "history = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    match = EloMatch(\n",
    "        home_team=row['home_team'],\n",
    "        away_team=row['away_team'],\n",
    "        home_goals=int(row['home_score']),\n",
    "        away_goals=int(row['away_score']),\n",
    "    )\n",
    "    new_home, new_away = elo.update(match)\n",
    "    \n",
    "    for team, rating in [(row['home_team'], new_home), (row['away_team'], new_away)]:\n",
    "        if team not in history:\n",
    "            history[team] = []\n",
    "        history[team].append({'date': row['kickoff'], 'elo': rating})\n",
    "\n",
    "# Classement final\n",
    "ranking = sorted(elo.ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "print('\\nClassement ELO final:\\n')\n",
    "for i, (team, rating) in enumerate(ranking, 1):\n",
    "    print(f'{i:2d}. {team:30s} {rating:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution ELO des top 5 et bottom 3\n",
    "top_teams = [t for t, _ in ranking[:5]]\n",
    "bottom_teams = [t for t, _ in ranking[-3:]]\n",
    "plot_teams = top_teams + bottom_teams\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "for team in plot_teams:\n",
    "    data = pd.DataFrame(history[team])\n",
    "    style = '-' if team in top_teams else '--'\n",
    "    ax.plot(data['date'], data['elo'], style, label=team, linewidth=1.5)\n",
    "\n",
    "ax.axhline(y=1500, color='gray', linestyle=':', alpha=0.5, label='Initial (1500)')\n",
    "ax.set_title('Evolution ELO - Ligue 1 2023-24')\n",
    "ax.set_ylabel('Rating ELO')\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_ROOT / 'data' / 'results' / 'elo_evolution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test de prediction 1X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions ELO sur les derniers matchs (in-sample)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Reconstruire ELO match par match et predire\n",
    "elo_test = EloRating(k_factor=32, home_advantage=100)\n",
    "\n",
    "for i, (_, row) in enumerate(df.iterrows()):\n",
    "    if i < 50:  # Besoin d'historique minimum\n",
    "        elo_test.update(EloMatch(\n",
    "            home_team=row['home_team'], away_team=row['away_team'],\n",
    "            home_goals=int(row['home_score']), away_goals=int(row['away_score']),\n",
    "        ))\n",
    "        continue\n",
    "    \n",
    "    # Predire AVANT de mettre a jour\n",
    "    pred = elo_test.predict_1x2(row['home_team'], row['away_team'])\n",
    "    predicted = max(pred, key=pred.get)\n",
    "    \n",
    "    hs, as_ = int(row['home_score']), int(row['away_score'])\n",
    "    actual = 'home' if hs > as_ else ('draw' if hs == as_ else 'away')\n",
    "    \n",
    "    if predicted == actual:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "    # Mettre a jour APRES\n",
    "    elo_test.update(EloMatch(\n",
    "        home_team=row['home_team'], away_team=row['away_team'],\n",
    "        home_goals=int(row['home_score']), away_goals=int(row['away_score']),\n",
    "    ))\n",
    "\n",
    "print(f'Accuracy ELO (predict-then-update): {correct/total:.1%} ({correct}/{total})')\n",
    "print(f'Baseline (toujours home win): {(df[\"home_score\"] > df[\"away_score\"]).mean():.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prochaine etape\n",
    "\n",
    "Les ratings ELO sont prets. On passe au notebook **04** pour l'ensemble, puis au **05** pour le backtest complet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
