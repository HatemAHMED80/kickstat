{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Ensemble & Calibration\n",
    "\n",
    "**Objectif** : Combiner Dixon-Coles + ELO et verifier la calibration des probabilites.\n",
    "\n",
    "## Questions cles :\n",
    "1. L'ensemble est-il meilleur que chaque modele seul ?\n",
    "2. Les probabilites sont-elles calibrees (70% predit = 70% reel) ?\n",
    "3. Quels poids optimaux pour l'ensemble ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from src.models.dixon_coles import DixonColesModel, MatchResult\n",
    "from src.models.elo import EloRating, EloMatch\n",
    "from src.models.ensemble import EnsemblePredictor\n",
    "from src.evaluation.calibration import evaluate, brier_score\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donnees\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "df = pd.read_csv(DATA_DIR / 'ligue1_2023.csv')\n",
    "df['kickoff'] = pd.to_datetime(df['kickoff'])\n",
    "df = df.sort_values('kickoff')\n",
    "print(f'Matchs: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fitter les deux modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Dixon-Coles\n",
    "dc_train = [\n",
    "    MatchResult(\n",
    "        home_team=row['home_team'],\n",
    "        away_team=row['away_team'],\n",
    "        home_goals=int(row['home_score']),\n",
    "        away_goals=int(row['away_score']),\n",
    "        date=row['kickoff'].to_pydatetime(),\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "dc = DixonColesModel()\n",
    "dc.fit(dc_train)\n",
    "print(f'Dixon-Coles fitte: {len(dc.teams)} equipes')\n",
    "\n",
    "# ELO\n",
    "elo = EloRating()\n",
    "for _, row in df.iterrows():\n",
    "    elo.update(EloMatch(\n",
    "        home_team=row['home_team'],\n",
    "        away_team=row['away_team'],\n",
    "        home_goals=int(row['home_score']),\n",
    "        away_goals=int(row['away_score']),\n",
    "    ))\n",
    "print(f'ELO fitte: {len(elo.ratings)} equipes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparer DC seul vs ELO seul vs Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predire sur les matchs d'entrainement (in-sample, juste pour comparer)\n",
    "dc_probs, elo_probs, ens_probs = [], [], []\n",
    "outcomes = []\n",
    "\n",
    "ensemble = EnsemblePredictor(dc, elo, dc_weight=0.65, elo_weight=0.35)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    h, a = row['home_team'], row['away_team']\n",
    "    if h not in dc.teams or a not in dc.teams:\n",
    "        continue\n",
    "    \n",
    "    # Dixon-Coles\n",
    "    dc_pred = dc.predict(h, a)\n",
    "    dc_probs.append([dc_pred.home_win, dc_pred.draw, dc_pred.away_win])\n",
    "    \n",
    "    # ELO\n",
    "    elo_pred = elo.predict_1x2(h, a)\n",
    "    elo_probs.append([elo_pred['home'], elo_pred['draw'], elo_pred['away']])\n",
    "    \n",
    "    # Ensemble\n",
    "    ens_pred = ensemble.predict(h, a)\n",
    "    ens_probs.append([ens_pred.home_prob, ens_pred.draw_prob, ens_pred.away_prob])\n",
    "    \n",
    "    # Outcome\n",
    "    hs, as_ = int(row['home_score']), int(row['away_score'])\n",
    "    outcomes.append(0 if hs > as_ else (1 if hs == as_ else 2))\n",
    "\n",
    "dc_arr = np.array(dc_probs)\n",
    "elo_arr = np.array(elo_probs)\n",
    "ens_arr = np.array(ens_probs)\n",
    "out_arr = np.array(outcomes)\n",
    "\n",
    "print('Brier Scores (in-sample, lower = better):')\n",
    "print(f'  Dixon-Coles: {brier_score(dc_arr, out_arr):.4f}')\n",
    "print(f'  ELO:         {brier_score(elo_arr, out_arr):.4f}')\n",
    "print(f'  Ensemble:    {brier_score(ens_arr, out_arr):.4f}')\n",
    "print(f'\\n(Rappel: c est du in-sample. Le vrai test est dans 05_backtest)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimisation des poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search sur les poids (in-sample)\n",
    "best_brier = 1.0\n",
    "best_w = 0.5\n",
    "results = []\n",
    "\n",
    "for w_dc in np.arange(0.3, 0.9, 0.05):\n",
    "    w_elo = 1.0 - w_dc\n",
    "    combined = dc_arr * w_dc + elo_arr * w_elo\n",
    "    # Renormaliser\n",
    "    combined = combined / combined.sum(axis=1, keepdims=True)\n",
    "    bs = brier_score(combined, out_arr)\n",
    "    results.append({'dc_weight': w_dc, 'elo_weight': w_elo, 'brier': bs})\n",
    "    if bs < best_brier:\n",
    "        best_brier = bs\n",
    "        best_w = w_dc\n",
    "\n",
    "df_w = pd.DataFrame(results)\n",
    "print(f'Meilleur poids DC: {best_w:.2f} (Brier: {best_brier:.4f})')\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(df_w['dc_weight'], df_w['brier'], 'o-')\n",
    "plt.axvline(x=best_w, color='red', linestyle='--', alpha=0.7, label=f'Optimal: {best_w:.2f}')\n",
    "plt.xlabel('Poids Dixon-Coles')\n",
    "plt.ylabel('Brier Score')\n",
    "plt.title('Optimisation des poids de l ensemble (in-sample)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Diagramme de calibration\n",
    "\n",
    "Quand on predit 70%, est-ce que ca arrive 70% du temps ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport complet de calibration pour l'ensemble\n",
    "report = evaluate(ens_arr, out_arr)\n",
    "print(report.summary())\n",
    "\n",
    "# Diagramme\n",
    "bins = report.calibration_bins\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Predicted vs Actual\n",
    "preds = [b['avg_predicted'] for b in bins if b['count'] > 0]\n",
    "actuals = [b['avg_actual'] for b in bins if b['count'] > 0]\n",
    "counts = [b['count'] for b in bins if b['count'] > 0]\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Calibration parfaite')\n",
    "ax1.scatter(preds, actuals, s=[c/2 for c in counts], alpha=0.7, c='steelblue')\n",
    "ax1.set_xlabel('Probabilite predite')\n",
    "ax1.set_ylabel('Frequence observee')\n",
    "ax1.set_title(f'Calibration (ECE = {report.ece:.3f})')\n",
    "ax1.legend()\n",
    "\n",
    "# Distribution des predictions\n",
    "ax2.bar(range(len(counts)), counts, color='steelblue', alpha=0.7)\n",
    "ax2.set_xlabel('Bin de probabilite')\n",
    "ax2.set_ylabel('Nombre de predictions')\n",
    "ax2.set_title('Distribution des predictions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_ROOT / 'data' / 'results' / 'calibration_insample.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prochaine etape\n",
    "\n",
    "L'ensemble est configure avec les poids optimaux. **Passons au notebook 05_backtest** pour la VRAIE validation walk-forward."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
