{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Walk-Forward Backtest\n",
    "\n",
    "## Le notebook le plus important du projet\n",
    "\n",
    "Ce notebook implémente le **backtest walk-forward** : la seule méthode honnête pour évaluer un modèle de prédiction sportive.\n",
    "\n",
    "### Qu'est-ce que le walk-forward backtesting ?\n",
    "\n",
    "Le walk-forward backtest simule exactement ce qui se passe en production :\n",
    "\n",
    "1. **Entraîner** le modèle sur toutes les données disponibles **avant** la journée cible\n",
    "2. **Prédire** les matchs de cette journée\n",
    "3. **Comparer** aux résultats réels après le match\n",
    "4. **Avancer** d'une journée et recommencer\n",
    "\n",
    "```\n",
    "Temps ──────────────────────────────────────►\n",
    "\n",
    "Fenêtre 1: [=====TRAIN=====][PRED]  \n",
    "Fenêtre 2: [======TRAIN======][PRED]  \n",
    "Fenêtre 3: [=======TRAIN=======][PRED]  \n",
    "...  \n",
    "```\n",
    "\n",
    "### Pourquoi c'est critique ?\n",
    "\n",
    "- **Pas de fuite de données** : le modèle ne voit jamais le futur\n",
    "- **Réaliste** : reproduit les conditions réelles de pari\n",
    "- **Fiable** : mesure la vraie capacité prédictive, pas le surapprentissage\n",
    "- **Décisionnel** : détermine si le modèle est prêt pour la production (GO/NO-GO)\n",
    "\n",
    "### Métriques évaluées\n",
    "\n",
    "| Métrique | Seuil GO | Description |\n",
    "|----------|----------|-------------|\n",
    "| Brier Score | < 0.22 | Précision globale des probabilités (0 = parfait, 0.25 = pile ou face) |\n",
    "| ECE | < 0.08 | Erreur de calibration attendue (les 70% arrivent-ils 70% du temps ?) |\n",
    "| ROI | > -2% | Retour sur investissement sur paris simulés |\n",
    "| Échantillon | >= 100 paris | Taille minimale pour significativité statistique |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Manipulation du sys.path pour importer depuis src/ ---\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Remonter au répertoire racine du projet\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Racine du projet : {PROJECT_ROOT}\")\n",
    "print(f\"Répertoire de données : {PROJECT_ROOT / 'data' / 'raw'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports standards ---\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Imports du projet ---\n",
    "from src.evaluation.backtest import WalkForwardBacktest, BacktestReport, BettingResult\n",
    "from src.evaluation.calibration import CalibrationReport\n",
    "\n",
    "# --- Configuration de l'affichage ---\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.1)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "pd.set_option(\"display.max_columns\", 30)\n",
    "pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n",
    "\n",
    "print(\"Imports chargés avec succès.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des données de matchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chargement des fichiers CSV depuis data/raw/ ---\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "csv_files = sorted(RAW_DIR.glob(\"*.csv\"))\n",
    "print(f\"Fichiers CSV trouvés dans {RAW_DIR} : {len(csv_files)}\")\n",
    "for f in csv_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"\\nAucun fichier CSV trouvé. Génération de données synthétiques pour démonstration...\")\n",
    "    print(\"En production, placez vos fichiers CSV dans data/raw/ avec les colonnes :\")\n",
    "    print(\"  home_team, away_team, home_score, away_score, kickoff, matchday\")\n",
    "    print(\"  + optionnel : home_odds, draw_odds, away_odds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_match_data(csv_files: list[Path]) -> pd.DataFrame:\n",
    "    \"\"\"Charger et consolider tous les CSV de matchs.\n",
    "    \n",
    "    Colonnes attendues : home_team, away_team, home_score, away_score, kickoff\n",
    "    Colonnes optionnelles : matchday, home_odds, draw_odds, away_odds\n",
    "    \"\"\"\n",
    "    if not csv_files:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    frames = []\n",
    "    for f in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            # Vérifier les colonnes obligatoires\n",
    "            required = {\"home_team\", \"away_team\", \"home_score\", \"away_score\", \"kickoff\"}\n",
    "            if not required.issubset(set(df.columns)):\n",
    "                print(f\"  ATTENTION: {f.name} manque des colonnes requises ({required - set(df.columns)})\")\n",
    "                continue\n",
    "            df[\"source_file\"] = f.name\n",
    "            frames.append(df)\n",
    "            print(f\"  Chargé {f.name} : {len(df)} matchs\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERREUR lors du chargement de {f.name} : {e}\")\n",
    "    \n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Concaténation et nettoyage\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    df[\"kickoff\"] = pd.to_datetime(df[\"kickoff\"], utc=True)\n",
    "    df = df.sort_values(\"kickoff\").reset_index(drop=True)\n",
    "    \n",
    "    # Supprimer les doublons potentiels\n",
    "    df = df.drop_duplicates(subset=[\"home_team\", \"away_team\", \"kickoff\"], keep=\"first\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_synthetic_data(n_matches: int = 400) -> pd.DataFrame:\n",
    "    \"\"\"Générer des données synthétiques pour démonstration.\n",
    "    \n",
    "    Simule une saison complète de Ligue 1 (20 équipes, 380 matchs).\n",
    "    Les données synthétiques permettent de tester le pipeline sans API.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    equipes = [\n",
    "        \"Paris Saint-Germain\", \"Olympique de Marseille\", \"AS Monaco\",\n",
    "        \"Olympique Lyonnais\", \"LOSC Lille\", \"OGC Nice\", \"RC Lens\",\n",
    "        \"Stade Rennais\", \"Montpellier HSC\", \"Toulouse FC\",\n",
    "        \"Stade de Reims\", \"FC Nantes\", \"RC Strasbourg\", \"Le Havre AC\",\n",
    "        \"FC Metz\", \"Clermont Foot\", \"FC Lorient\", \"Stade Brestois\",\n",
    "        \"Angers SCO\", \"AJ Auxerre\"\n",
    "    ]\n",
    "    \n",
    "    # Forces relatives des équipes (attaque / défense)\n",
    "    forces = {\n",
    "        equipes[0]: (2.2, 0.7), equipes[1]: (1.5, 0.9), equipes[2]: (1.6, 0.85),\n",
    "        equipes[3]: (1.4, 0.95), equipes[4]: (1.3, 0.85), equipes[5]: (1.2, 0.9),\n",
    "        equipes[6]: (1.1, 0.85), equipes[7]: (1.1, 0.95), equipes[8]: (1.0, 1.0),\n",
    "        equipes[9]: (0.95, 1.0), equipes[10]: (0.9, 0.9), equipes[11]: (0.9, 1.05),\n",
    "        equipes[12]: (0.85, 1.0), equipes[13]: (0.8, 1.1), equipes[14]: (0.8, 1.15),\n",
    "        equipes[15]: (0.75, 1.2), equipes[16]: (0.75, 1.15), equipes[17]: (0.9, 0.95),\n",
    "        equipes[18]: (0.7, 1.2), equipes[19]: (0.7, 1.15),\n",
    "    }\n",
    "    \n",
    "    matchs = []\n",
    "    date_debut = datetime(2024, 8, 10)\n",
    "    journee = 0\n",
    "    \n",
    "    for i, home in enumerate(equipes):\n",
    "        for j, away in enumerate(equipes):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if len(matchs) >= n_matches:\n",
    "                break\n",
    "            \n",
    "            att_h, def_h = forces[home]\n",
    "            att_a, def_a = forces[away]\n",
    "            \n",
    "            # Simulation Poisson avec avantage domicile\n",
    "            lambda_h = 1.35 * att_h * def_a * 1.25\n",
    "            lambda_a = 1.35 * att_a * def_h\n",
    "            \n",
    "            home_score = np.random.poisson(lambda_h)\n",
    "            away_score = np.random.poisson(lambda_a)\n",
    "            \n",
    "            journee = len(matchs) // 10 + 1\n",
    "            date_match = date_debut + pd.Timedelta(days=journee * 7 + np.random.randint(0, 3))\n",
    "            \n",
    "            # Générer des cotes simulées (avec marge bookmaker ~5%)\n",
    "            p_home = max(0.10, min(0.85, lambda_h / (lambda_h + lambda_a) * 0.55 + 0.15))\n",
    "            p_draw = max(0.15, 0.28 - abs(p_home - 0.5) * 0.3)\n",
    "            p_away = 1.0 - p_home - p_draw\n",
    "            \n",
    "            marge = 1.05  # 5% de marge bookmaker\n",
    "            home_odds = round(marge / max(p_home, 0.05), 2)\n",
    "            draw_odds = round(marge / max(p_draw, 0.05), 2)\n",
    "            away_odds = round(marge / max(p_away, 0.05), 2)\n",
    "            \n",
    "            matchs.append({\n",
    "                \"home_team\": home,\n",
    "                \"away_team\": away,\n",
    "                \"home_score\": int(home_score),\n",
    "                \"away_score\": int(away_score),\n",
    "                \"kickoff\": date_match.isoformat() + \"Z\",\n",
    "                \"matchday\": journee,\n",
    "                \"home_odds\": home_odds,\n",
    "                \"draw_odds\": draw_odds,\n",
    "                \"away_odds\": away_odds,\n",
    "            })\n",
    "        if len(matchs) >= n_matches:\n",
    "            break\n",
    "    \n",
    "    df = pd.DataFrame(matchs)\n",
    "    df[\"kickoff\"] = pd.to_datetime(df[\"kickoff\"], utc=True)\n",
    "    df = df.sort_values(\"kickoff\").reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Charger les données réelles ou générer des données synthétiques ---\n",
    "df_matches = load_match_data(csv_files)\n",
    "\n",
    "if df_matches.empty:\n",
    "    print(\"Utilisation des données synthétiques pour démonstration.\")\n",
    "    df_matches = generate_synthetic_data(n_matches=400)\n",
    "    DATA_SOURCE = \"Synthétique (démonstration)\"\n",
    "else:\n",
    "    DATA_SOURCE = \"Réel (data/raw/)\"\n",
    "\n",
    "print(f\"\\nSource des données : {DATA_SOURCE}\")\n",
    "print(f\"Nombre total de matchs : {len(df_matches)}\")\n",
    "print(f\"Période : {df_matches['kickoff'].min()} → {df_matches['kickoff'].max()}\")\n",
    "print(f\"Équipes uniques : {df_matches['home_team'].nunique()}\")\n",
    "print(f\"\\nColonnes disponibles : {list(df_matches.columns)}\")\n",
    "\n",
    "display(df_matches.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Préparer les données au format attendu par WalkForwardBacktest ---\n",
    "\n",
    "# Convertir le DataFrame en liste de dicts pour le backtest\n",
    "all_matches = df_matches.to_dict(orient=\"records\")\n",
    "\n",
    "# Préparer le dictionnaire de cotes (si disponibles)\n",
    "odds_data = None\n",
    "has_odds = all(\n",
    "    col in df_matches.columns\n",
    "    for col in [\"home_odds\", \"draw_odds\", \"away_odds\"]\n",
    ")\n",
    "\n",
    "if has_odds:\n",
    "    odds_data = {}\n",
    "    for _, row in df_matches.iterrows():\n",
    "        match_key = f\"{row['home_team']}_vs_{row['away_team']}\"\n",
    "        odds_data[match_key] = {\n",
    "            \"home_odds\": row[\"home_odds\"],\n",
    "            \"draw_odds\": row[\"draw_odds\"],\n",
    "            \"away_odds\": row[\"away_odds\"],\n",
    "        }\n",
    "    print(f\"Cotes disponibles pour {len(odds_data)} matchs → calcul des edges activé\")\n",
    "else:\n",
    "    print(\"Pas de cotes disponibles → backtest sans simulation de paris\")\n",
    "    print(\"Colonnes de cotes manquantes. Ajoutez home_odds, draw_odds, away_odds aux CSV.\")\n",
    "\n",
    "print(f\"\\nMatchs prêts pour le backtest : {len(all_matches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exécution du Walk-Forward Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration du backtest ---\n",
    "# min_training_matches : nombre minimum de matchs avant la première prédiction\n",
    "# min_edge_pct : seuil minimum d'edge pour simuler un pari (en %)\n",
    "# dc_weight / elo_weight : pondération de l'ensemble Dixon-Coles vs ELO\n",
    "\n",
    "backtest = WalkForwardBacktest(\n",
    "    min_training_matches=100,\n",
    "    min_edge_pct=5.0,\n",
    "    dc_weight=0.65,\n",
    "    elo_weight=0.35,\n",
    ")\n",
    "\n",
    "print(\"Configuration du backtest :\")\n",
    "print(f\"  Matchs d'entraînement minimum : {backtest.min_training}\")\n",
    "print(f\"  Edge minimum pour pari : {backtest.min_edge}%\")\n",
    "print(f\"  Poids Dixon-Coles : {backtest.dc_weight}\")\n",
    "print(f\"  Poids ELO : {backtest.elo_weight}\")\n",
    "print(f\"  Matchs à prédire : {len(all_matches) - backtest.min_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Lancement du backtest walk-forward ---\n",
    "# C'est l'étape la plus longue : chaque match est prédit après\n",
    "# entraînement sur tous les matchs précédents\n",
    "\n",
    "print(\"Démarrage du backtest walk-forward...\")\n",
    "print(\"(Cela peut prendre plusieurs minutes selon le nombre de matchs)\\n\")\n",
    "\n",
    "report: BacktestReport = backtest.run(\n",
    "    all_matches=all_matches,\n",
    "    odds_data=odds_data,\n",
    ")\n",
    "\n",
    "print(\"\\nBacktest terminé !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Résultats du backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Affichage du rapport complet ---\n",
    "print(report.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tableau détaillé des métriques de calibration ---\n",
    "cal = report.calibration\n",
    "\n",
    "metriques = pd.DataFrame([\n",
    "    {\"Métrique\": \"Brier Score\", \"Valeur\": f\"{cal.brier_score:.4f}\", \"Seuil GO\": \"< 0.22\", \"Statut\": \"PASS\" if cal.brier_score < 0.22 else \"FAIL\"},\n",
    "    {\"Métrique\": \"Log Loss\", \"Valeur\": f\"{cal.log_loss:.4f}\", \"Seuil GO\": \"-\", \"Statut\": \"-\"},\n",
    "    {\"Métrique\": \"ECE\", \"Valeur\": f\"{cal.ece:.4f}\", \"Seuil GO\": \"< 0.08\", \"Statut\": \"PASS\" if cal.ece < 0.08 else \"FAIL\"},\n",
    "    {\"Métrique\": \"Accuracy\", \"Valeur\": f\"{cal.accuracy:.1%}\", \"Seuil GO\": \"-\", \"Statut\": \"-\"},\n",
    "    {\"Métrique\": \"N prédictions\", \"Valeur\": f\"{cal.n_predictions}\", \"Seuil GO\": \"-\", \"Statut\": \"-\"},\n",
    "])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MÉTRIQUES DE CALIBRATION\")\n",
    "print(\"=\" * 60)\n",
    "display(metriques.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tableau détaillé des résultats de paris ---\n",
    "paris_metriques = pd.DataFrame([\n",
    "    {\"Métrique\": \"Edges trouvés (>5%)\", \"Valeur\": f\"{report.total_edges_found}\"},\n",
    "    {\"Métrique\": \"Paris simulés\", \"Valeur\": f\"{len(report.betting_results)}\"},\n",
    "    {\"Métrique\": \"Taux de réussite\", \"Valeur\": f\"{report.win_rate:.1%}\"},\n",
    "    {\"Métrique\": \"Edge moyen\", \"Valeur\": f\"{report.avg_edge:.1f}%\"},\n",
    "    {\"Métrique\": \"ROI (mise fixe)\", \"Valeur\": f\"{report.roi:.2%}\"},\n",
    "    {\"Métrique\": \"PnL total\", \"Valeur\": f\"{sum(b.pnl for b in report.betting_results):.2f} unités\"},\n",
    "])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RÉSULTATS DES PARIS SIMULÉS\")\n",
    "print(\"=\" * 60)\n",
    "display(paris_metriques.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Détail des paris par marché ---\n",
    "if report.betting_results:\n",
    "    df_bets = pd.DataFrame([\n",
    "        {\n",
    "            \"date\": b.match_date,\n",
    "            \"match\": f\"{b.home_team} vs {b.away_team}\",\n",
    "            \"marché\": b.market,\n",
    "            \"prob_modèle\": b.model_prob,\n",
    "            \"prob_bookmaker\": b.fair_bookmaker_prob,\n",
    "            \"edge_%\": b.edge_pct,\n",
    "            \"cote\": b.best_odds,\n",
    "            \"résultat\": b.actual_outcome,\n",
    "            \"gagné\": b.won,\n",
    "            \"pnl\": b.pnl,\n",
    "        }\n",
    "        for b in report.betting_results\n",
    "    ])\n",
    "    df_bets = df_bets.sort_values(\"date\").reset_index(drop=True)\n",
    "    \n",
    "    # Résumé par marché\n",
    "    print(\"\\nRésumé par type de marché :\")\n",
    "    resume_marche = df_bets.groupby(\"marché\").agg(\n",
    "        n_paris=(\"pnl\", \"count\"),\n",
    "        taux_reussite=(\"gagné\", \"mean\"),\n",
    "        edge_moyen=(\"edge_%\", \"mean\"),\n",
    "        pnl_total=(\"pnl\", \"sum\"),\n",
    "        roi=(\"pnl\", \"mean\"),\n",
    "    ).round(4)\n",
    "    display(resume_marche)\n",
    "    \n",
    "    print(f\"\\nDerniers 10 paris simulés :\")\n",
    "    display(df_bets.tail(10))\n",
    "else:\n",
    "    print(\"Aucun pari simulé (pas de cotes disponibles ou aucun edge détecté).\")\n",
    "    df_bets = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Diagramme de calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diagramme de calibration : probabilité prédite vs fréquence réelle ---\n",
    "# Un modèle parfaitement calibré suit la diagonale y = x\n",
    "\n",
    "bins = report.calibration.calibration_bins\n",
    "\n",
    "# Filtrer les bins avec des observations\n",
    "bins_non_vides = [b for b in bins if b[\"count\"] > 0]\n",
    "\n",
    "if bins_non_vides:\n",
    "    avg_pred = [b[\"avg_predicted\"] for b in bins_non_vides]\n",
    "    avg_actual = [b[\"avg_actual\"] for b in bins_non_vides]\n",
    "    counts = [b[\"count\"] for b in bins_non_vides]\n",
    "    gaps = [b[\"gap\"] for b in bins_non_vides]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "    # --- Graphique 1 : Calibration ---\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    # Diagonale parfaite\n",
    "    ax1.plot([0, 1], [0, 1], \"k--\", linewidth=1.5, alpha=0.6, label=\"Calibration parfaite\")\n",
    "    \n",
    "    # Points de calibration (taille proportionnelle au nombre d'observations)\n",
    "    sizes = np.array(counts)\n",
    "    sizes_normalized = 50 + 300 * (sizes / max(sizes))  # Normaliser la taille des points\n",
    "    \n",
    "    scatter = ax1.scatter(\n",
    "        avg_pred, avg_actual,\n",
    "        s=sizes_normalized,\n",
    "        c=gaps,\n",
    "        cmap=\"RdYlGn_r\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=0.8,\n",
    "        alpha=0.85,\n",
    "        zorder=5,\n",
    "    )\n",
    "    \n",
    "    # Ligne reliant les points\n",
    "    ax1.plot(avg_pred, avg_actual, \"-\", color=\"#2196F3\", linewidth=1.5, alpha=0.5, zorder=4)\n",
    "    \n",
    "    # Zone acceptable (ECE < 0.08)\n",
    "    x_fill = np.linspace(0, 1, 100)\n",
    "    ax1.fill_between(x_fill, x_fill - 0.08, x_fill + 0.08,\n",
    "                     alpha=0.1, color=\"green\", label=\"Zone acceptable (ECE < 0.08)\")\n",
    "    \n",
    "    cbar = plt.colorbar(scatter, ax=ax1, shrink=0.8)\n",
    "    cbar.set_label(\"Écart (gap)\", fontsize=10)\n",
    "    \n",
    "    ax1.set_xlabel(\"Probabilité prédite\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Fréquence réelle\", fontsize=12)\n",
    "    ax1.set_title(\n",
    "        f\"Diagramme de Calibration\\n\"\n",
    "        f\"ECE = {report.calibration.ece:.4f} | Brier = {report.calibration.brier_score:.4f}\",\n",
    "        fontsize=13, fontweight=\"bold\",\n",
    "    )\n",
    "    ax1.set_xlim(-0.02, 1.02)\n",
    "    ax1.set_ylim(-0.02, 1.02)\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    ax1.legend(loc=\"upper left\", fontsize=9)\n",
    "\n",
    "    # --- Graphique 2 : Histogramme des observations par bin ---\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    bin_centers = [(b[\"bin_start\"] + b[\"bin_end\"]) / 2 for b in bins]\n",
    "    bin_width = bins[0][\"bin_end\"] - bins[0][\"bin_start\"]\n",
    "    all_counts = [b[\"count\"] for b in bins]\n",
    "    \n",
    "    colors = [\"#4CAF50\" if b[\"gap\"] < 0.05 else \"#FF9800\" if b[\"gap\"] < 0.10 else \"#F44336\"\n",
    "              for b in bins]\n",
    "    \n",
    "    ax2.bar(bin_centers, all_counts, width=bin_width * 0.85,\n",
    "            color=colors, edgecolor=\"black\", linewidth=0.5, alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlabel(\"Probabilité prédite (bin)\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Nombre d'observations\", fontsize=12)\n",
    "    ax2.set_title(\"Distribution des prédictions par bin\", fontsize=13, fontweight=\"bold\")\n",
    "    \n",
    "    # Légende manuelle pour les couleurs\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=\"#4CAF50\", edgecolor=\"black\", label=\"Gap < 5%\"),\n",
    "        Patch(facecolor=\"#FF9800\", edgecolor=\"black\", label=\"Gap 5-10%\"),\n",
    "        Patch(facecolor=\"#F44336\", edgecolor=\"black\", label=\"Gap > 10%\"),\n",
    "    ]\n",
    "    ax2.legend(handles=legend_elements, loc=\"upper right\", fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(PROJECT_ROOT / \"data\" / \"results\" / \"calibration_diagram.png\"),\n",
    "                dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Diagramme sauvegardé dans data/results/calibration_diagram.png\")\n",
    "else:\n",
    "    print(\"Pas assez de données de calibration pour tracer le diagramme.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PnL cumulé au fil du temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Graphique du PnL cumulé sur la durée du backtest ---\n",
    "# Ce graphique montre l'évolution du profit/perte cumulé(e)\n",
    "# Une courbe montante = modèle profitable\n",
    "\n",
    "if not df_bets.empty and len(df_bets) > 0:\n",
    "    df_pnl = df_bets.sort_values(\"date\").copy()\n",
    "    df_pnl[\"pnl_cumulé\"] = df_pnl[\"pnl\"].cumsum()\n",
    "    df_pnl[\"pari_n\"] = range(1, len(df_pnl) + 1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    # Remplissage vert/rouge selon le signe du PnL\n",
    "    ax.fill_between(\n",
    "        df_pnl[\"pari_n\"], df_pnl[\"pnl_cumulé\"], 0,\n",
    "        where=df_pnl[\"pnl_cumulé\"] >= 0,\n",
    "        color=\"#4CAF50\", alpha=0.15, interpolate=True,\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        df_pnl[\"pari_n\"], df_pnl[\"pnl_cumulé\"], 0,\n",
    "        where=df_pnl[\"pnl_cumulé\"] < 0,\n",
    "        color=\"#F44336\", alpha=0.15, interpolate=True,\n",
    "    )\n",
    "    \n",
    "    # Courbe principale\n",
    "    ax.plot(\n",
    "        df_pnl[\"pari_n\"], df_pnl[\"pnl_cumulé\"],\n",
    "        color=\"#1565C0\", linewidth=2.0, zorder=5,\n",
    "    )\n",
    "    \n",
    "    # Ligne de zéro\n",
    "    ax.axhline(y=0, color=\"black\", linewidth=0.8, linestyle=\"-\", alpha=0.5)\n",
    "    \n",
    "    # Annotations clés\n",
    "    pnl_final = df_pnl[\"pnl_cumulé\"].iloc[-1]\n",
    "    pnl_max = df_pnl[\"pnl_cumulé\"].max()\n",
    "    pnl_min = df_pnl[\"pnl_cumulé\"].min()\n",
    "    drawdown_max = (df_pnl[\"pnl_cumulé\"].cummax() - df_pnl[\"pnl_cumulé\"]).max()\n",
    "    \n",
    "    # Point final\n",
    "    couleur_final = \"#4CAF50\" if pnl_final >= 0 else \"#F44336\"\n",
    "    ax.scatter([len(df_pnl)], [pnl_final], color=couleur_final, s=100, zorder=10,\n",
    "               edgecolors=\"black\", linewidth=1)\n",
    "    ax.annotate(\n",
    "        f\"PnL final : {pnl_final:+.2f}u\",\n",
    "        xy=(len(df_pnl), pnl_final),\n",
    "        xytext=(15, 15), textcoords=\"offset points\",\n",
    "        fontsize=11, fontweight=\"bold\", color=couleur_final,\n",
    "        arrowprops=dict(arrowstyle=\"->\", color=couleur_final, lw=1.5),\n",
    "    )\n",
    "    \n",
    "    # Informations dans un encadré\n",
    "    textstr = (\n",
    "        f\"ROI : {report.roi:.2%}\\n\"\n",
    "        f\"Win rate : {report.win_rate:.1%}\\n\"\n",
    "        f\"Max drawdown : {drawdown_max:.2f}u\\n\"\n",
    "        f\"N paris : {len(df_bets)}\"\n",
    "    )\n",
    "    props = dict(boxstyle=\"round,pad=0.5\", facecolor=\"wheat\", alpha=0.8)\n",
    "    ax.text(0.02, 0.97, textstr, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment=\"top\", bbox=props)\n",
    "    \n",
    "    ax.set_xlabel(\"Numéro du pari\", fontsize=12)\n",
    "    ax.set_ylabel(\"PnL cumulé (unités)\", fontsize=12)\n",
    "    ax.set_title(\n",
    "        f\"Profit & Loss Cumulé - Walk-Forward Backtest\\n\"\n",
    "        f\"Mise fixe 1 unité | Edge minimum {backtest.min_edge}%\",\n",
    "        fontsize=13, fontweight=\"bold\",\n",
    "    )\n",
    "    ax.yaxis.set_major_formatter(mticker.FormatStrFormatter(\"%.1f\"))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(PROJECT_ROOT / \"data\" / \"results\" / \"cumulative_pnl.png\"),\n",
    "                dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Graphique sauvegardé dans data/results/cumulative_pnl.png\")\n",
    "else:\n",
    "    print(\"Pas de paris simulés disponibles pour tracer le PnL cumulé.\")\n",
    "    print(\"Assurez-vous que les données contiennent des cotes (home_odds, draw_odds, away_odds).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Distribution des edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Histogramme de la distribution des edges détectés ---\n",
    "# L'edge = (prob_modèle - prob_bookmaker) / prob_bookmaker * 100\n",
    "# Un edge positif signifie que notre modèle pense que l'événement\n",
    "# est plus probable que ce que le bookmaker propose\n",
    "\n",
    "if not df_bets.empty and len(df_bets) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    edges = df_bets[\"edge_%\"].values\n",
    "    edges_gagnants = df_bets[df_bets[\"gagné\"]][\"edge_%\"].values\n",
    "    edges_perdants = df_bets[~df_bets[\"gagné\"]][\"edge_%\"].values\n",
    "    \n",
    "    # --- Graphique 1 : Distribution globale des edges ---\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    ax1.hist(edges, bins=25, color=\"#2196F3\", edgecolor=\"black\",\n",
    "             linewidth=0.5, alpha=0.7, label=\"Tous les paris\")\n",
    "    \n",
    "    ax1.axvline(x=np.mean(edges), color=\"#F44336\", linewidth=2, linestyle=\"--\",\n",
    "                label=f\"Moyenne : {np.mean(edges):.1f}%\")\n",
    "    ax1.axvline(x=np.median(edges), color=\"#FF9800\", linewidth=2, linestyle=\"-.\",\n",
    "                label=f\"Médiane : {np.median(edges):.1f}%\")\n",
    "    \n",
    "    ax1.set_xlabel(\"Edge (%)\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Nombre de paris\", fontsize=12)\n",
    "    ax1.set_title(\"Distribution des Edges Détectés\", fontsize=13, fontweight=\"bold\")\n",
    "    ax1.legend(fontsize=9)\n",
    "    \n",
    "    # --- Graphique 2 : Edges gagnants vs perdants ---\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    bins_range = np.linspace(edges.min(), edges.max(), 20)\n",
    "    \n",
    "    if len(edges_gagnants) > 0:\n",
    "        ax2.hist(edges_gagnants, bins=bins_range, color=\"#4CAF50\", edgecolor=\"black\",\n",
    "                 linewidth=0.5, alpha=0.6, label=f\"Gagnants (n={len(edges_gagnants)})\")\n",
    "    if len(edges_perdants) > 0:\n",
    "        ax2.hist(edges_perdants, bins=bins_range, color=\"#F44336\", edgecolor=\"black\",\n",
    "                 linewidth=0.5, alpha=0.6, label=f\"Perdants (n={len(edges_perdants)})\")\n",
    "    \n",
    "    ax2.set_xlabel(\"Edge (%)\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Nombre de paris\", fontsize=12)\n",
    "    ax2.set_title(\"Edges : Gagnants vs Perdants\", fontsize=13, fontweight=\"bold\")\n",
    "    ax2.legend(fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(PROJECT_ROOT / \"data\" / \"results\" / \"edge_distribution.png\"),\n",
    "                dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Graphique sauvegardé dans data/results/edge_distribution.png\")\n",
    "    \n",
    "    # Statistiques complémentaires\n",
    "    print(f\"\\nStatistiques des edges :\")\n",
    "    print(f\"  Min    : {edges.min():.1f}%\")\n",
    "    print(f\"  Max    : {edges.max():.1f}%\")\n",
    "    print(f\"  Moyenne: {edges.mean():.1f}%\")\n",
    "    print(f\"  Médiane: {np.median(edges):.1f}%\")\n",
    "    print(f\"  Écart-type: {edges.std():.1f}%\")\n",
    "else:\n",
    "    print(\"Pas de paris simulés pour tracer la distribution des edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Décision GO / NO-GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Évaluation GO / NO-GO pour mise en production ---\n",
    "# Critères stricts pour décider si le modèle est prêt\n",
    "\n",
    "# Vérification de chaque critère\n",
    "criteres = [\n",
    "    {\n",
    "        \"critère\": \"Brier Score < 0.22\",\n",
    "        \"valeur\": f\"{report.calibration.brier_score:.4f}\",\n",
    "        \"seuil\": 0.22,\n",
    "        \"ok\": report.calibration.brier_score < 0.22,\n",
    "        \"description\": \"Précision globale des probabilités\",\n",
    "    },\n",
    "    {\n",
    "        \"critère\": \"ECE < 0.08\",\n",
    "        \"valeur\": f\"{report.calibration.ece:.4f}\",\n",
    "        \"seuil\": 0.08,\n",
    "        \"ok\": report.calibration.ece < 0.08,\n",
    "        \"description\": \"Erreur de calibration attendue\",\n",
    "    },\n",
    "    {\n",
    "        \"critère\": \"ROI > -2%\",\n",
    "        \"valeur\": f\"{report.roi:.2%}\",\n",
    "        \"seuil\": -0.02,\n",
    "        \"ok\": report.roi > -0.02,\n",
    "        \"description\": \"Retour sur investissement\",\n",
    "    },\n",
    "    {\n",
    "        \"critère\": \"Échantillon >= 100 paris\",\n",
    "        \"valeur\": f\"{len(report.betting_results)} paris\",\n",
    "        \"seuil\": 100,\n",
    "        \"ok\": len(report.betting_results) >= 100,\n",
    "        \"description\": \"Taille minimale pour significativité\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Décision finale\n",
    "tous_ok = all(c[\"ok\"] for c in criteres)\n",
    "decision = \"GO\" if tous_ok else \"NO-GO\"\n",
    "couleur_decision = \"\\033[92m\" if tous_ok else \"\\033[91m\"  # Vert ou rouge ANSI\n",
    "reset = \"\\033[0m\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"  DÉCISION GO / NO-GO POUR MISE EN PRODUCTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df_criteres = pd.DataFrame(criteres)\n",
    "df_criteres[\"statut\"] = df_criteres[\"ok\"].map({True: \"PASS\", False: \"FAIL\"})\n",
    "display(\n",
    "    df_criteres[[\"critère\", \"valeur\", \"statut\", \"description\"]]\n",
    "    .style\n",
    "    .hide(axis=\"index\")\n",
    "    .map(\n",
    "        lambda v: \"background-color: #C8E6C9\" if v == \"PASS\" else \"background-color: #FFCDD2\" if v == \"FAIL\" else \"\",\n",
    "        subset=[\"statut\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"{couleur_decision}  >>> DÉCISION FINALE : {decision} <<<{reset}\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "if tous_ok:\n",
    "    print(\"\\n  Le modèle satisfait tous les critères. Prêt pour un déploiement\")\n",
    "    print(\"  progressif en production avec surveillance continue.\")\n",
    "else:\n",
    "    print(\"\\n  Le modèle ne satisfait pas tous les critères.\")\n",
    "    print(\"  Actions recommandées :\")\n",
    "    for c in criteres:\n",
    "        if not c[\"ok\"]:\n",
    "            print(f\"    - {c['critère']} : valeur actuelle = {c['valeur']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualisation synthétique de la décision GO/NO-GO ---\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "labels = [c[\"critère\"] for c in criteres]\n",
    "statuts = [c[\"ok\"] for c in criteres]\n",
    "couleurs = [\"#4CAF50\" if s else \"#F44336\" for s in statuts]\n",
    "symboles = [\"PASS\" if s else \"FAIL\" for s in statuts]\n",
    "\n",
    "bars = ax.barh(labels, [1] * len(labels), color=couleurs, edgecolor=\"black\",\n",
    "               linewidth=0.8, alpha=0.8, height=0.6)\n",
    "\n",
    "# Ajouter les valeurs et statuts sur les barres\n",
    "for i, (bar, critere) in enumerate(zip(bars, criteres)):\n",
    "    ax.text(\n",
    "        0.5, bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{critere['valeur']}  [{symboles[i]}]\",\n",
    "        ha=\"center\", va=\"center\", fontsize=12, fontweight=\"bold\",\n",
    "        color=\"white\",\n",
    "    )\n",
    "\n",
    "# Bannière de décision\n",
    "couleur_bg = \"#4CAF50\" if tous_ok else \"#F44336\"\n",
    "ax.text(\n",
    "    0.5, 1.08, f\"DÉCISION : {decision}\",\n",
    "    transform=ax.transAxes, ha=\"center\", va=\"center\",\n",
    "    fontsize=18, fontweight=\"bold\", color=\"white\",\n",
    "    bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=couleur_bg, edgecolor=\"black\"),\n",
    ")\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xticks([])\n",
    "ax.set_title(\"Tableau de bord GO / NO-GO\", fontsize=14, fontweight=\"bold\", pad=30)\n",
    "ax.invert_yaxis()  # Premier critère en haut\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(PROJECT_ROOT / \"data\" / \"results\" / \"go_nogo_dashboard.png\"),\n",
    "            dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Tableau de bord sauvegardé dans data/results/go_nogo_dashboard.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Résumé et prochaines étapes\n",
    "\n",
    "### Ce que nous avons fait\n",
    "\n",
    "1. **Chargé les données** de matchs depuis `data/raw/` (ou généré des données synthétiques)\n",
    "2. **Exécuté le backtest walk-forward** : entraînement incrémental sur le passé, prédiction du futur\n",
    "3. **Évalué la calibration** : Brier Score, Log Loss, ECE\n",
    "4. **Simulé des paris** : identification des edges et suivi du PnL\n",
    "5. **Pris une décision GO/NO-GO** basée sur des seuils objectifs\n",
    "\n",
    "### Interprétation des résultats\n",
    "\n",
    "- **Brier Score** : mesure la distance entre les probabilités prédites et les résultats réels. Un score < 0.22 indique que le modèle bat un pronostiqueur naïf (qui prédirait toujours les probabilités historiques).\n",
    "\n",
    "- **ECE** : vérifie que les probabilités sont bien calibrées. Quand le modèle dit \"70%\", cela arrive-t-il vraiment ~70% du temps ? Un ECE < 0.08 est acceptable.\n",
    "\n",
    "- **ROI** : le retour sur investissement sur des paris simulés à mise fixe. Un ROI > -2% signifie que les pertes sont contenues et que le modèle a un potentiel avec une meilleure stratégie de mise.\n",
    "\n",
    "- **Taille de l'échantillon** : au minimum 100 paris pour avoir une signification statistique. En dessous, les résultats peuvent être dus au hasard.\n",
    "\n",
    "### Prochaines étapes\n",
    "\n",
    "| Étape | Description | Priorité |\n",
    "|-------|-------------|----------|\n",
    "| Données réelles | Remplacer les données synthétiques par les données football-data.org | Haute |\n",
    "| Cotes historiques | Intégrer les cotes historiques pour un calcul d'edge réaliste | Haute |\n",
    "| Optimisation des poids | Grid search sur dc_weight / elo_weight via cross-validation | Moyenne |\n",
    "| Stratégie de mise | Tester Kelly criterion vs mise fixe vs mise proportionnelle | Moyenne |\n",
    "| Ajout de features | xG, forme récente, blessures, weather pour le modèle ML | Basse |\n",
    "| Monitoring live | Tableau de bord en temps réel avec dérive de calibration | Basse |\n",
    "\n",
    "### Règle d'or\n",
    "\n",
    "> **Ne jamais déployer un modèle qui n'a pas passé le backtest walk-forward.**  \n",
    "> Les backtests \"classiques\" (train/test split fixe) surestiment systématiquement la performance.  \n",
    "> Seul le walk-forward reproduit les conditions réelles d'utilisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sauvegarde des résultats du backtest ---\n",
    "results_dir = PROJECT_ROOT / \"data\" / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sauvegarder le rapport en JSON\n",
    "import json\n",
    "\n",
    "rapport_dict = {\n",
    "    \"date_execution\": datetime.now().isoformat(),\n",
    "    \"source_donnees\": DATA_SOURCE,\n",
    "    \"config\": {\n",
    "        \"min_training_matches\": backtest.min_training,\n",
    "        \"min_edge_pct\": backtest.min_edge,\n",
    "        \"dc_weight\": backtest.dc_weight,\n",
    "        \"elo_weight\": backtest.elo_weight,\n",
    "    },\n",
    "    \"calibration\": {\n",
    "        \"brier_score\": report.calibration.brier_score,\n",
    "        \"log_loss\": report.calibration.log_loss,\n",
    "        \"ece\": report.calibration.ece,\n",
    "        \"accuracy\": report.calibration.accuracy,\n",
    "        \"n_predictions\": report.calibration.n_predictions,\n",
    "        \"is_acceptable\": report.calibration.is_acceptable,\n",
    "    },\n",
    "    \"paris\": {\n",
    "        \"total_edges\": report.total_edges_found,\n",
    "        \"n_paris\": len(report.betting_results),\n",
    "        \"win_rate\": report.win_rate,\n",
    "        \"avg_edge\": report.avg_edge,\n",
    "        \"roi\": report.roi,\n",
    "        \"pnl_total\": sum(b.pnl for b in report.betting_results),\n",
    "    },\n",
    "    \"decision\": decision,\n",
    "    \"criteres\": [{k: v for k, v in c.items() if k != \"seuil\"} for c in criteres],\n",
    "}\n",
    "\n",
    "rapport_path = results_dir / \"backtest_report.json\"\n",
    "with open(rapport_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rapport_dict, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"Rapport sauvegardé : {rapport_path}\")\n",
    "\n",
    "# Sauvegarder les paris détaillés en CSV\n",
    "if not df_bets.empty:\n",
    "    bets_path = results_dir / \"backtest_bets.csv\"\n",
    "    df_bets.to_csv(bets_path, index=False)\n",
    "    print(f\"Détail des paris sauvegardé : {bets_path}\")\n",
    "\n",
    "print(f\"\\nDécision finale : {decision}\")\n",
    "print(\"Backtest terminé.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
